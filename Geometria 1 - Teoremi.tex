\documentclass[9pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}

\usepackage[a4paper,top=3.cm,bottom=3.cm,left=3cm,right=3cm]{geometry}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
	\hskip -\arraycolsep
	\let\@ifnextchar\new@ifnextchar
	\array{#1}}
\makeatother
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{cancel}
\usepackage{framed}

\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\letvs}{Sia $ V $ uno spazio vettoriale sul campo $ \K $}
\newcommand{\letvsR}{Sia $ V $ uno spazio vettoriale sul campo $ \R $}
\newcommand{\letvsC}{Sia $ V $ uno spazio vettoriale sul campo $ \C $}
\newcommand{\letlin}{Sia $ L \colon \K^n \to \K^n $ un'applicazione lineare}
\newcommand{\Ker}{\mathrm{Ker}\,}
\newcommand{\Imm}{\mathrm{Im}\,}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\rg}{\mathrm{rg}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\cof}{\mathrm{cof}}
\newcommand{\scprd}[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\Id}{\mathrm{Id}}

\newtheoremstyle{mythm}{\topsep}{\topsep}{\rmfamily}{}{\bfseries}{}{.5em}{}

\theoremstyle{mythm}
\newtheorem{axiom}{Assioma}[section]
\newtheorem{definition}{Definizione}[section]
\newtheorem{propriety}{Proprietà}[section]
\newtheorem{thm}{Teorema}[section]
\newtheorem{corollary}[thm]{Corollario}
\newtheorem{prop}[thm]{Proposizione}

\title{\textsc{Geometria 1}}
\author{Alessandro Piazza \thanks{alessandro.piazza@sns.it}}


\begin{document}

\maketitle

\begin{abstract}
	Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
\end{abstract}

\tableofcontents

\clearpage

\section{Spazi Vettoriali}
\begin{definition}[campo]
	Un campo $ \K $ è un insieme su cui sono definite due operazioni di \emph{somma} e $ + $ e \emph{prodotto} $ \cdot $ che associano a due elementi dell'insieme un altro elemento dell'insieme
	\begin{enumerate}
		\item $ \forall x, y \in \K \Rightarrow x + y \in \K $
		\item $ \forall x, y \in \K \Rightarrow x \cdot y \in \K $
	\end{enumerate}
	e che rispettano le seguenti proprietà
	\begin{enumerate}[label=(\roman*)]
		\item \emph{associativa}
		\item \emph{commutativa}
		\item \emph{esistenza degli elementi neutri}
		\item \emph{opposto}
		\item \emph{inverso}
		\item \emph{distributiva del prodotto rispetto alla sommma}
	\end{enumerate}
\end{definition}

\begin{definition}[spazio vettoriale]
	Uno spazio vettoriale su un campo $ \K $ è un insieme  $ V $ su cui sono definite due operazioni di 
	\begin{enumerate}
		\item \emph{somma} tra elementi di $ V $: $ \forall v, w \in V \Rightarrow v + w \in V $
		\item \emph{prodotto per scalare}: $ \forall v \in V, \forall \lambda \in \K \Rightarrow\lambda v \in V $
	\end{enumerate}
	che devono rispettare le seguenti proprietà:
	\begin{enumerate}[label=(\roman*)]
		\item \emph{associativa della somma}
		\item \emph{commutativa della somma}
		\item \emph{esistenza dell'elemento neutro della somma}
		\item \emph{opposto della somma}
		\item \emph{distributiva del prodotto rispetto alla somma}
		\item \emph{associativa del prodotto}
		\item \emph{elemento neutro del prodotto}
	\end{enumerate}
	Chiameremo \emph{vettori} gli elementi di $ V $.
\end{definition}

\begin{propriety} Uno spazio vettoriale gode delle seguenti proprietà:
	\begin{enumerate}
		\item Unicità dell'elemento neutro
		\item Unicità dell'opposto
		\item $ 0 \cdot v = O_V $
		\item $ (-1) \cdot v = -v $
	\end{enumerate}
\end{propriety}

\begin{definition}[sottospazio vettoriale]
	\letvs. Diciamo che $ W \subseteq V $ è un sottospazio vettoriale di $ V $ se valgono le seguenti proprietà
	\begin{enumerate}[label=(\roman*)]
		\item $ \forall v, w \in W \Rightarrow v + w \in W $
		\item $ \forall v \in W, \forall \lambda \in \K \Rightarrow \lambda v \in W $
		\item $ O_V \in W $
	\end{enumerate}
\end{definition}

\clearpage

\begin{thm}[intersezione di sottospazi]
	\textsf{Intersezione di sottospazi è un sottospazio vettoriale}
\end{thm}

\begin{thm}[somma di sottospazi]
	\textsf{Somma di sottospazi è un sottospazio vettoriale}
\end{thm}

\begin{definition}[combinazione lineare]
	\letvs, siano $ v_1, \ldots , v_n \in \nobreak V $ e $ \lambda_1, \ldots , \lambda_n \in \K $. Si dice combinazione lineare dei $ v_i $ un vettore $ v \in V $ tale che \[v = \sum_{i=1}^{n} \lambda_i v_i = \lambda_1 v_1 + \ldots + \lambda_n v_n \]
\end{definition}

\begin{definition}[span]
	\[Span\{v_1, \ldots , v_n\} = \left\{ v \in V: \exists \lambda_1, \ldots , \lambda_n \in \K: v = \sum_{i=1}^{n} \lambda_i v_i \right\}\]
\end{definition}

\begin{thm}[proprietà dello span]
	\letvs \, e siano $ v_1, \ldots , v_n \in \nolinebreak V $. Allora $ Span\{v_1, \ldots , v_n\} $ è il più piccolo sottospazio vettoriale di $ V $ che contiene tutti i $ v_i $.
\end{thm}

\begin{definition}[dipendenza e indpendenza lineare] 
	\letvs \, e siano $ v_1, \ldots , v_n \in V $. Si dice che $ v_1, \ldots, v_n $ sono linearmente dipendenti se $ \exists \lambda_1, \ldots , \lambda_n \in \K $ non tutti nulli tali che \[\lambda_1 v_1 + \ldots + \lambda_n v_n = O_V.\] Analogamente si dice che $ v_1, \ldots, v_n $ sono linearmente indipendenti se \[\lambda_1 v_1 + \ldots + \lambda_n v_n = O_V \Leftrightarrow \lambda_1 = \ldots = \lambda_n = 0.\]	
\end{definition}

\begin{definition}[base]
	\letvs. Un insieme $ \{v_1, \ldots, v_n\} $ si dice base di $ V $ se
	\begin{enumerate}[label=(\roman*)]
		\item $ v_1, \ldots, v_n $ sono linearmente indipendenti
		\item $ Span\{v_1, \ldots , v_n\} = V $
	\end{enumerate}
\end{definition}

\begin{thm}[unicità della combinazione lineare]
	contenuto...
\end{thm}

\begin{definition}[sottoinsieme massimale]
	\textsf{se aggiungo un vettore l'insieme non è più linearmente indipendente}	
\end{definition}

\begin{thm}
	\textsf{linearmente indipendente + massimale = base}
\end{thm}

\begin{thm}
	\letvs \ e sia $ \{v_1, \ldots, v_m\} $ una base di $ V $. Se $ w_1, \ldots, w_n $, con $ n > m $, sono vettori di $ V $, allora $ w_1, \ldots, w_n $ sono linermente indipendenti.
\end{thm}

\begin{corollary}
	\letvs. Supponiamo di avere due basi di $ V $, una con $ n $ elementi e una con $ m $ elementi. Allora $ n = m $.
\end{corollary}

\begin{definition}[dimensione]
	\letvs \, avente una base costituita da $ n $ vettori. Allora diremo che $ V $ ha dimensione $ n $ e scriveremo $ \dim V = n $.
\end{definition}

\begin{thm}
	\textsf{sono n + generano = base}
\end{thm}

\begin{thm}
	\textsf{sono n + set massimale = base}
\end{thm}

\begin{thm}
	\textsf{sono n + linearmente indipendenti = base}
\end{thm}

\begin{thm}[completamento ad una base]
	\letvs \, di dimensione $ n \geq 2 $. Sia $ r $ un intero positivo con $ 0 < r < n $. Dati $ r $ vettori $ v_1, \ldots , v_r \in V $ linearmente indipendenti è possibile completarli ad una base di $ V $, ossia trovare vettori $ v_{r+1}, \ldots, v_n $ tali che $ \{v_1, \ldots , v_r, v_{r+1}, \ldots , v_n\} $ è base di $ V $.
\end{thm}

\clearpage

\section{Matrici}

\begin{definition}[matrice]
	Una matrice $ m \times n $ a coefficienti in $ \K $ è un tabella ordinata di $ m $ righe e $ n $ colonne i cui elementi appartengono ad un campo $ \K $. Dati $ a_{ij} \in \K $ con $ i = 1, \ldots, m $ e $ j = 1, \ldots, n $ diremo che $ A \in \mathrm{Mat}_{m \times n} (\K) $ e scriveremo
	\[ A = 
	\begin{pmatrix}
		a_{11} & \cdots  & a_{1n} \\
		\vdots & \ddots & \vdots \\
		a_{m1} & \cdots  & a_{mn} \\
	\end{pmatrix}\]
\end{definition}

\begin{definition}[matrice diagonale e identità]
	contenuto...
\end{definition}

\begin{definition}[prodotto tra matrici]
	contenuto...
\end{definition}

\begin{propriety}
	\textsf{proprietà del prodotto ($ n \times n $ stabile rispetto al prodotto)}
\end{propriety}

\begin{definition}[matrice trasposta]
	contenuto...
\end{definition}

\begin{propriety}[della trasposta]
	contenuto...
\end{propriety}

\begin{definition}[matrici coniugate]
	Due matrici $ A $ e $ B $ si dicono coinugate se esiste una matrice $ P $ invertibile tale che \[B = P^{-1} A P.\] Matrici coniugate rappresentano la stessa applicazioni lineari viste in due basi diverse. 
\end{definition}

\begin{definition}[traccia]
	Sia $ M $ una matrice quadrata $ n \times n $. La traccia di $ M $ è la somma degli elementi sulla diagonale \[ \tr(M) = \tr 
	\begin{pmatrix}
	a_{11} & \cdots  & a_{1n} \\
	\vdots & \ddots & \vdots \\
	a_{n1} & \cdots  & a_{nn} \\
	\end{pmatrix}
	= a_{11} + \ldots + a_{nn}\]
\end{definition}

\begin{propriety}[della traccia]
	La traccia gode delle seguenti proprietà
	\begin{enumerate}[label=(\roman*)]
		\item $ \tr (A + B) = \tr(A) + \tr(B) $ e $ \tr(\lambda A) = \lambda \, \tr (A) $
		\item $ \tr(\prescript{t}{}{A}) = \tr (A) $
		\item $ \tr (AB) = \tr (BA) $
	\end{enumerate}
\end{propriety}

\begin{thm}[invarianza della traccia per coniugio]
	Se $ A $ e $ B $ sono matrici coniugate, allora $ \tr (A) = \tr (B) $
\end{thm}

Roba su riduzione a scalini...

\clearpage

\section{Applicazioni lineari}

\begin{definition}[applicazione lineare]
	Siano $ V $ e $ W $ due spazi vettoriali su un campo $ \K $. Diremo che una funzione $ L \colon V \to W  $ è un'applicazione lineare (o mappa lineare) se $ L $ soddisfa le seguenti proprietà
	\begin{enumerate}[label=(\roman*)]
		\item $ \forall v_1, v_2 \in V $ vale $ L(v_1 + v_2) = L(v_1) + L(v_2) $
		\item $ \forall v \in V, \forall \lambda \in \K $ vale $ L(\lambda v) = \lambda L(v)$
	\end{enumerate}
	Diretta conseguenza è la seguente proprietà chiave delle applicazioni lineari
	\begin{enumerate}[resume, label=(\roman*)]
		\item $ L(O_V) =O_W $
	\end{enumerate}
\end{definition}

\begin{definition}[nucleo]
	Siano $ V $ e $ W $ due spazi vettoriali su un campo $ \K $ e sia $ L \colon V \to W  $ un'applicazione lineare. Definiamo nucleo o kernel di $ L $, e scriveremo $ \Ker L$, l'insieme degli elementi di $ V $ la cui immagine attraverso $ L $ è lo zero di $ W $. Formalmente \[\Ker L = \{v \in V \colon L(v) = O_W\} \]
\end{definition}

\begin{thm}
	Sia $ L \colon V \to W  $ un'applicazione lineare. Allora $ \Ker L $ è un sottospazio vettoriale di $ V $.
\end{thm}

\begin{thm}
	Sia $ L \colon V \to W  $ un'applicazione lineare. Allora $ \Imm L $ è un sottospazio vettoriale di $ W $.
\end{thm}

\begin{thm}
	Un'applicazione lineare $ L \colon V \to W $ è iniettiva se e solo se $ \Ker L = \{O_V\} $.
\end{thm}

\begin{thm}[composizione di applicazioni lineari]
	\textsf{la composizione di due applicazioni lineari è ancora un'applicazione lineare}
\end{thm}

\begin{thm}[inversa di un'applicazione lineare]
	\textsf{l'inversa di una applicazioni lineare (se esiste) è ancora un'applicazione lineare}
\end{thm}

\begin{thm}
	\letlin \, tale che $ \Ker L = \{O_V\} $. Se $ v_1, \ldots, v_n \in V $ sono vettori linearmente indipendenti, anche $ L(v_1), \ldots, L(v_n) $ sono vettori linearmente indipendenti di $ W $.
\end{thm}

\begin{thm}[delle dimensioni]
	Siano $ V $ e $ W $ spazi vettoriali su un campo $ \K $ e sia $ L \colon V \to W $  un'applicazione lineare. Allora vale \[\dim V = \dim \Imm L + \dim \Ker L\]
\end{thm}

\begin{thm}
	\letlin. Se $ \Ker L = \{O_V\} $ e $ \Imm L = W $, allora $ L $ è biettiva e dunque invertibile
\end{thm}

\begin{definition}[isomorfismo]
	Un'applicazione lineare $ L \colon V \to W $ biettiva si dice isomorfismo. Se tale applicazioni esiste si dice che $ V $ e $ W $ sono isomorfi.
\end{definition}

\begin{definition}[endomorfismo]
	Un'applicazione lineare $ L \colon V \to V $ dallo spazio in sé si dice endomorfismo.
\end{definition}

\clearpage

\section{Applicazioni lineari e matrici}

\begin{thm}
	Sia $ L \colon \K^n \to \K $ un'applicazione lineare. Allora esiste un unico vettore $ A \in \K^n $ tale che $ \forall X \in \K^n $ \[L(X) = A \cdot X\]
\end{thm}

\begin{thm}
	Esiste una corrispondenza biunivoca tra \[\{L \colon \K^n \to \K^m \quad \mathrm{lineare}\} \leftrightarrow \mathrm{Mat}_{m \times n}(\K)\]
	\begin{enumerate}
		\item Data una matrice $ M \in \mathrm{Mat}_{m \times n}(\K) $ è possibile associare ad essa un'applicazione lineare
		\begin{align*}
		L \colon \K^n & \to \K^m \\
		X & \mapsto MX
		\end{align*}
		\item Sia $ L \colon \K^n \to \K^m $ un'applicazione lineare. Allora esiste una matrice $ M \in \mathrm{Mat}_{m \times n}(\K) $ tale che $ \forall X \in \K^n $, $ L(X) = MX $. Se $ \mathscr{C} = \{e_1, \ldots , e_n\} $ la è base canonica di $ \K^n $, le colonne di $ M $ sono $ L(e_1), \ldots, L(e_n) $.
	\end{enumerate}
\end{thm}

\begin{thm}
	Siano $ V $ e $ W $ spazi vettoriali su un campo $ \K $ e sia $ L \colon \K^n \to \K^m $ un'applicazione lineare. Siano inoltre $ \mathscr{B} = \{v_1, \ldots, v_n\} $ e $ \mathscr{B}' = \{w_1, \ldots, w_m\} $ basi rispettivamente di $ V $ e di $ W $. Preso $ v \in V $ esiste una matrice $ M \in \mathrm{Mat}_{m \times n}(\K) $ tale che \[X_{\mathscr{B} '} (L(v)) = M X_{\mathscr{B}} (v) \] dove $ X(w) $ è il vettore colonna di $ w $ scritto nella rispettiva base. 
\end{thm}

\begin{thm}
	Siano $ V $ e $ W $ due spazi vettoriali su $ \K $ di dimensione $ n $ e $ m $. Siano $ \mathscr{B} = \{v_1, \ldots, v_n\} $ e $ \mathscr{B}' = \{w_1, \ldots, w_m\} $ basi rispettivamente di $ V $ e di $ W $. \\ Indicando con $ \mathscr{L}(V, W) = \{L \colon \K^n \to \K^m \quad \mathrm{lineare}\} $ e con $ \left [L \right ]_{\mathscr{B}'} ^{\mathscr{B}} $ la matrice associata a $ L $ rispetto alle basi $ \mathscr{B} $ e $ \mathscr{B}' $, si ha che 
	\begin{align*}
	M \colon \mathscr{L}(V, W) & \to \mathrm{Mat}_{m \times n}(\K)\\
	L & \mapsto \left [L \right ]_{\mathscr{B}'} ^{\mathscr{B}}
	\end{align*}
	è un'applicazione lineare ed è un isomorfismo tra lo spazio delle applicazioni lineari e lo spazio delle matrici. 
\end{thm}

\begin{thm}[matrice di funizione composta]
	Siano $ V $, $ W $ e $ U $ spazi vettoriali e siano \linebreak $ \mathscr{B} = \{v_1, \ldots, v_n\} $, $ \mathscr{B}' = \{w_1, \ldots, w_m\} $ e $ \mathscr{B}'' = \{u_1, \ldots, u_s\} $ basi di $ V $, $ W $ e $ U $ rispettivamente. Siano inoltre $ F \colon V \to W $ e $ G \colon W \to U $ lineari. Allora \[ \left [G \circ F \right ]_{\mathscr{B}''} ^{\mathscr{B}} = \left [G \right ]_{\mathscr{B}''} ^{\mathscr{B}'} \left [F \right ]_{\mathscr{B}'} ^{\mathscr{B}} \]
\end{thm}

\begin{thm}
	Sia $ L \colon V \to W $ un'applicazione lineare e $ B \colon V \to V $ lineare e invertibile. Allora vale che \[\Imm L \circ B = \Imm L \quad \mathrm{e} \quad \dim \Ker L \circ B = \dim  \Ker L\] In altre parole se $ \left [L \right ] $ è la matrice associata a $ L $ e $ \left [B \right ] $ è la matrice invertibile delle mosse di colonna associata a $ B $, la matrice $ \left [B \right ] \left [L \right ] $ è una matrice ridotta a scalini per colonna in cui lo $ Span $ delle colonne è lo stesso dello span delle colonne di $ \left [L \right ] $. Più brevemente la riduzione di Gauss per colonne lascia invariato lo $ Span $ delle colonne. 
\end{thm}

\begin{thm}
	Sia $ L \colon V \to W $ un'applicazione lineare e $ U \colon W \to W $ lineare e invertibile. Allora vale che \[\Ker U \circ L = \Ker L\quad \mathrm{e} \quad \dim \Imm U \circ L = \dim \Imm L  \] In altre parole se $ \left [L \right ] $ è la matrice associata a $ L $ e $ \left [U \right ] $ è la matrice invertibile delle mosse di riga associata a $ U $, la matrice $ \left [L \right ] \left [U \right ] $ è una matrice ridotta a scalini per riga che ha lo stesso $ \Ker $ di $ \left [L \right ] $. Più brevemente la riduzione di Gauss per righe lascia invariato lo spazio delle soluzioni di un sistema lineare omogeneo.
\end{thm}

\begin{definition}[rango]
	Sia $ A \in \mathrm{Mat}_{m \times n} (\K) $ definiamo rango di A, $ \mathrm{rg} A $, in modo equivalente come
	\begin{enumerate}
		\item il numero massimo di colonne linearmente indipendenti (numero di \emph{pivot} colonna di $ A $ ridotta a scalini per colonna)
		\item il numero massimo di righe linearmente indipendenti (numero di \emph{pivot} riga di $ A $ ridotta a scalini per righe)
		\item la $ \dim \Imm L $, dove $ L \colon \K^n \to \K^m $ è l'applicazione lineare associata $ \left [L \right ]_{\mathscr{B}'} ^{\mathscr{B}} = A $
	\end{enumerate}
\end{definition}

\clearpage

\section{Formula di Grassmann e somma diretta}

\begin{thm}[formula di Grassmann]
	Siano $ A $ e $ B $ sottospazi vettoriali di $ V $ su un campo $ \K $. Vale \[\dim A + \dim B = \dim (A + B) + \dim (A \cap B)\]
\end{thm}

\begin{definition}[somma diretta]
	Dati $ A $ e $ B $ sottospazi di $ V $ su un campo $ \K $, si dice che $ A $ e $ B $ sono in somma diretta se $ A \cap B = \{O_V\} $.\\
	In modo del tutto equivalente $ A $ e $ B $ sono in somma diretta se e solo se $ \dim A \, + \, \dim B = \dim (A + B) $.	
\end{definition}

\begin{definition}[somma diretta di $ k $ sottospazi]
	$ U_1, \ldots , U_k $ sottospazi di $ V $ su un campo $ \K $ si dicono essere insomma diretta se $ \forall i \in \{1, \ldots, k\} $ vale \[U_i \cap (U_1 + \ldots + \hat{U}_i + \ldots + U_k) = \{O_V\}\]
	In modo equivalente $ U_1, \ldots , U_k $ sono insomma diretta se e solo se \[\dim U_1 + \ldots + \dim U_k = \dim (U_1 + \ldots + U_k)\]
\end{definition}

\begin{definition}[complementare di un sottospazio]
	Sia $ A $ un sottospazio di $ V $ su un campo $ \K $. Un complementare di $ A $ è un sottospazio $ B $ di $ V $ tale che
	\begin{enumerate}[label=(\roman*)]
		\item $ A \cap B = \{O_V\} $ ($ A $ e $ B $ sono in somma diretta)
		\item $ A + B = V $
	\end{enumerate}
	In tal caso scriveremo che $ A \oplus B = V $. 
\end{definition}

\clearpage

\section{Sistemi lineari}

\begin{definition}[sistema lineare omogeneo]
	contenuto...
\end{definition}

\begin{definition}[matrice associata al sistema lineare omogeneo]
	contenuto...
\end{definition}

\begin{thm}[dimensione delle soluzioni]
	Sia $ M $ la matrice associata ad un sistema lineare omogeneo con $ n $ incognite. Indicando con $ S $ lo spazio delle soluzioni del sistema lineare vale \[\dim S = n - \mathrm{rg}M\]
\end{thm}

\begin{definition}[sottospazio ortogonale]
	contenuto... + \textsf{il sottospazio ortogonale è l'inieme delle soluzioni del sistema omogeneo}
\end{definition}

\begin{definition}[sistema lineare non omogeneo]
	contenuto...
\end{definition}

\begin{definition}[matrice completa e incompleta associata]
	contenuto...
\end{definition}

\begin{thm}[insieme soluzioni del sistema non omogeneo]
	Sia $ S $ l'insieme delle soluzioni del sistema non omogeneo e $ S_0 $ l'insieme delle soluzioni del sistema omogeneo associato. Supposto $ S \neq \emptyset $, preso un qualunque $ v \in S $ vale \[S = v + S_0\]
\end{thm}

\begin{definition}[sottospazio affine]
	\letvs, $ U $ un suo sottospazio e $ v \in V - U $ ($ v \neq 0 $) si dice che l'insieme $ v + U $ è un sottospazio affine di $ V $. Per convenzione si pone $ \dim (v + U) = \dim U $.
\end{definition}

\clearpage

\section{Prodotti scalari}

\begin{definition}[prodotto scalare]
	\letvs. Un prodotto scalare su $ V $ è una funzione \[\scprd{\;}{\,} \colon V \times V \to \K \] che soddisfa le seguenti prorpietà
	\begin{enumerate}[label=(\roman*)]
		\item $ \forall v,w \in V $ vale $ \scprd{v}{w} = \scprd{w}{v} $
		\item $ \forall v, w, u \in V $ vale $ \scprd{v}{w + u} = \scprd{v}{w} + \scprd{v}{u} $
		\item $ \forall v, w \in V, \forall \lambda \in \K $ vale $ \scprd{\lambda v}{w} = \scprd{v}{\lambda w} = \lambda \scprd{v}{w} $
	\end{enumerate}	
\end{definition}

\begin{propriety} Alcuni prodotti scalari godono delle segenti proprietà
	\begin{enumerate}
		\item Un vettore $ v \in V $ tale che $ \scprd{v}{v} = 0 $ si dice \emph{isotropo}.
		\item Un prodotto scalare tale che preso un $ v \in V $ \[\forall w \in V \quad \scprd{v}{w} = O_V \Rightarrow v = O_V\] si dice \emph{non degenere}.
		\item Un prodotto scalare su $ V $ spazio vettoriale sul campo $ \R $ tale che \[ \forall v \in V \quad \scprd{v}{v} \geq 0 \quad \mathrm{e} \quad \scprd{v}{v} = 0 \Leftrightarrow v = O_V \] si dice \emph{definito positivo}.
	\end{enumerate}
\end{propriety}

\begin{thm}
	Un prodotto scalare è non degenere se e solo se la matrice $ E $ che lo rappresenta ha rango massimo. Ovvero, se $ \{e_1, \ldots, e_n \} $ è base di $ V $
	\[ \mathrm{rg} E = \mathrm{rg} 
	\begin{pmatrix}
		\scprd{e_1}{e_1} & \cdots  & \scprd{e_1}{e_n} \\
		\vdots           & \ddots & \vdots \\
		\scprd{e_n}{e_1} & \cdots  & \scprd{e_n}{e_n} \\
	\end{pmatrix}
	= n	\]
\end{thm}

\begin{definition}[norma, distanza, ortogonalità]
	Sia $ \scprd{\,}{} $ un prodotto scalare su $ V $ spazio vettoriale su $ \R $. Allora
	\begin{enumerate}
		\item la norma di $ v \in V $ è $ \norm{v} = \sqrt{\scprd{v}{v}} $
		\item la distanza tra $ v, w \in V $ è data da $ \norm{v - w} $
		\item due vettori $ v, w \in V $ si dicono ortogonali se $ \scprd{v}{w} = 0 $
	\end{enumerate}
\end{definition}

\begin{thm}[di Pitagora]
	\letvsR \, con prodotto scalare definito positivo. Dati $ v, w \in V \colon \scprd{v}{w} = 0 $	allora \[\norm{v + w}^2 = \norm{v}^2 + \norm{w}^2\]
\end{thm}

\begin{thm}[del parallelogramma]
	\letvsR \, con prodotto scalare definito positivo. Allora $ \forall v, w \in V $ vale \[\norm{v + w}^2 + \norm{v - w}^2 = 2\norm{v}^2 + 2\norm{w}^2\]
\end{thm}

\begin{definition}[componente]
	Dati $ v, w \in V $ chiamiamo coefficiente di Fourier o componente di $ v $ lungo $ w $ lo scalare \[c = \frac{\scprd{v}{w}}{\scprd{w}{w}}\] In particolare vale $ \scprd{v - cw}{w} = 0 $
\end{definition}

\begin{thm}[disuguaglianza di Schwarz]
	\letvsR \, con prodotto scalare definito positivo. Allora $ \forall v, w \in V $ vale \[|\scprd{v}{w}| \leq \norm{v} \cdot \norm{w}\]
\end{thm}

\begin{thm}[disuguaglianza triangolare]
	\letvsR \, con prodotto scalare definito positivo. Allora $ \forall v, w \in V $ vale \[\norm{v + w} \leq \norm{v} + \norm{w}\]
\end{thm}

\begin{thm}
	Siano $ v_1, \ldots , v_n \in V $ a due a due perpendicolari ($ \forall i \neq j \; \scprd{v_i}{v_j} = 0 $). Allora $ \forall v \in V $ il vettore \[v - \frac{\scprd{v}{v_1}}{\scprd{v_1}{v_1}}v_1 - \ldots - \frac{\scprd{v}{v_n}}{\scprd{v_n}{v_n}}v_n\] è ortogonale a ciascuno dei $ v_i $. Risulta inoltre che il vettore $ c_1 v_1 + \ldots + c_n v_n $ (dove $ c_i = \frac{\scprd{v}{v_i}}{\scprd{v_i}{v_i}} $) è la migliore approssimazione di $ v $ come combinazione lineare dei $ v_i $.
\end{thm}

\begin{thm}[disuguaglianza di Bessel]
	Siano $ e_1, \ldots , e_n \in V $ a due a due perpendicolari e unitari. Dato un $ v \in V $, sia $ c_i = \frac{\scprd{v}{e_i}}{\scprd{e_i}{e_i}} $. Allora vale \[\sum_{i = 1}^{n}c_i^2 \leq \norm{v}^2\]
\end{thm}

\begin{thm}[ortogonalizzazione di Gram-Schmidt]
	Sia $ V $ uno spazio vettoriale con prodotto scalare definito positivo. Siano $ v_1, \ldots , v_n \in V $ vettori linearmente indipendenti. Possiamo allora trovare dei vettori $ u_1,\ldots, u_r $, con $ r \leq n $ ortogonali tra loro e tali che $ \forall i \leq r, \, Span\{v_1, \ldots, v_i\} = Span \{u_1, \ldots, u_i\} $. In particolare basterà procedere in modo induttivo e prendere
	\[\begin{cases}
	u_1 = v_1 \\
	u_{i} = v_i - \frac{\scprd{v_i}{u_1}}{\scprd{u_1}{u_1}}u_1 - \ldots - \frac{\scprd{v_i}{u_{i-1}}}{\scprd{u_{i-1}}{u_{i-1}}}u_{i-1}
	\end{cases}\]
\end{thm}

\begin{corollary}[esistenza della base ortonormale per prodotto scalare definito positivo]
	Dato $ V $ spazio vettoriale con prodotto scalare definito positivo esiste una base ortonormale di $ V $, ossia una base $ \{u_1,\ldots, u_r\} $ tale che $ \forall i \neq j \; \scprd{u_i}{u_j} = 0 $ e che $ \forall i \; \norm{u_i} = 1 $.
\end{corollary}

\begin{thm}
	Sia $ V $ uno spazio vettoriale con prodotto scalare definito positivo. Per ogni $ W $ sottospazio di $ V $ vale che \[V = W \oplus W^{\perp}\]
\end{thm}

\begin{definition}[prodotto hermitiano]
	\letvsC. Un prodotto hermitiano su $ V $ è una funzione \[\scprd{\;}{\,} \colon V \times V \to \K \] che soddisfa le seguenti proprietà
	\begin{enumerate}[label=(\roman*)]
		\item $ \forall v,w \in V $ vale $ \scprd{v}{w} = \overline{\scprd{w}{v}} $ (coniugato)
		\item $ \forall v, w, u \in V $ vale $ \scprd{v}{w + u} = \scprd{v}{w} + \scprd{v}{u} $ e $ \scprd{v + w}{u} = \scprd{v}{u} + \scprd{w}{u} $
		\item $ \forall v, w \in V, \forall \lambda \in \K $ vale $ \scprd{\lambda v}{w} = \lambda \scprd{v}{w} $ e $ \scprd{v}{\lambda w} = \overline{\lambda} \scprd{v}{w} $
	\end{enumerate}	
\end{definition}

\begin{definition}[prodotto hermitiano standard]
	Dati due vettori colonna $ v, w \in \C^n $ definiamo il prodotto hermitiano standard come 
	\[v \cdot w = 
	\begin{pmatrix}
	\alpha_1 \\
	\vdots \\
	\alpha_n \\ 
	\end{pmatrix}
	\cdot 
	\begin{pmatrix}
	\beta_1 \\
	\vdots \\
	\beta_n \\ 
	\end{pmatrix}
	 = \alpha_1 \overline{\beta_1} + \ldots + \alpha_n \overline{\beta_n}\]
\end{definition}

\begin{thm}[esistenza della base ortogonale per prodotto scalare generico]
	\letvs \ non banale di dimensione finita dotato di un prodotto scalare. Allora $ V $ ha una base ortogonale. 
\end{thm}

\begin{prop}[base ortonormale]
	Sia $ \{w_1, \ldots, w_n\} $ una base ortogonale di $ V $ con un prodotto scalare. Posto
	\[v_i = 
	\begin{cases*}
	\frac{w_i}{\sqrt{\scprd{w_i}{w_i}}} & se $ \scprd{w_i}{w_i} > 0 $ \\
	w_i & se $ \scprd{w_i}{w_i} = 0 $ \\
	\frac{w_i}{\sqrt{-\scprd{w_i}{w_i}}} & se $ \scprd{w_i}{w_i} < 0 $ \\
	\end{cases*}\]
	l'insieme $ \{v_1, \ldots, v_n\} $ è una base ortonormale di $ V $. 
\end{prop}

\begin{thm}[corrispondenza matrice - prodotto scalare]
	\letvs \ e sia $ \mathscr{B} = \{v_1, \ldots, v_n\} $ base di $ V $. Dato un prodotto scale $ \varphi \colon V \times V \to \K $ la matrice del prodotto scalare è \[M_{\mathscr{B}}(\varphi) = (\varphi(v_i, v_j))_{\substack{i = 1, \ldots, n \\ j = 1, \ldots, n}}.\] Viceversa data $ M $ matrice simmetrica del prodotto scalare e $ u, w $ vettori di vettori colonna $ [u]_{\mathscr{B}} $ e $ [w]_{\mathscr{B}} $ si ha \[\varphi(v, w) = [u]_{B}^t \cdot M \cdot [w]_{\mathscr{B}}\].
\end{thm}

\begin{prop}[radicale]
	Vale che $ V_0 = \{v \in V : \forall w \in V, \varphi(v, w) = 0\} = \{v \in V : M_{\mathscr{B}}(\varphi) \cdot [v]_{\mathscr{B}} = 0\} $. (Moralmente $ V_0 = \ker{M_{\mathscr{B}}(\varphi)} $). 
\end{prop}

\begin{definition}[spazio duale, funzionali]
	\letvs. Si definisce spazio duale di $ V $ l'insieme delle applicazioni lineari da $ V $ in $ \K $ \[V^{*} = \mathscr{L}(V, \K) = \mathscr{L}(V) = \{L \colon V \to \K : L \text{ è lineare}\}.\] I suoi elementi vengono detti funzionali lineari da $ V $ in $ \K $ e risulta $ \dim V = \dim V^{*} $. 
\end{definition}

\begin{definition}[base duale]
	\letvs. Fissata una base $ \{v_1, \ldots, v_n\} $ di $ V $ esiste una base $ \{\varphi_1, \ldots, \varphi_n\} $ di $ V^{*} $ ad essa associata detta base duale di $ v_1, \ldots, v_n $ definita come 
	\[\varphi_i (v_j) = 
	\begin{cases*}
	1 & se $ i = j $ \\
	0 & se $ i \neq j $ \\
	\end{cases*}\]
\end{definition}

\begin{thm} \label{thm:isom_duale}
	\letvs \ di dimensione finita con un prodotto scalare non degenere. Allora l'applicazione
	\begin{align*}
	\Phi \colon V & \to V^{*} \\
	v & \mapsto L_v = \scprd{v}{\cdot \,}
	\end{align*}
	dove $ L_v $ è la funzionale tale che $ \forall w \in V : L_v(w) = \scprd{v}{w} $, è un isomorfismo tra $ V $ e il suo duale $ V^{*} $. 
\end{thm}

\begin{thm}[annullatore]
	\letvs \ di $ \dim V = n $ e sia $ W $ sottospazio di $ V $. Sia inoltre $ W^{\circ} = \{\varphi \in V^{*} : \forall w \in W, \varphi(w) = 0\} $ l'annullatore di $ W $. Allora $ W^{\circ} $ è sottospazio di $ V^{*} $ e vale che $ \dim W^{\circ} = n - \dim W $. 
\end{thm}

\begin{corollary}
	\letvs \ di $ \dim V = n $ con prodotto scalare non degenere, sia $ W $ sottospazio di $ V $ e $ W^{\perp} $ il suo ortogonale. Siano inoltre $ V^{*} $ il duale di $ V $ e $ W^{\circ} $ l'annullatore di $ W $. Allora $ \Phi(W^{\perp}) = W^{\circ} $ (in altre parole $ \Phi|_{W^{\perp}} \colon W^{\perp} \to W^{\circ} $ è un isomorfismo) e vale quindi \[\dim W + \dim W^{\perp} = \dim V\]. 
\end{corollary}

\begin{thm}
	Sia $ \Phi \colon V \to V^{*} $ l'isomorfismo del Teorema \ref{thm:isom_duale}. Detto $ V_0 $ il radicale di $ V $ vale che $ V_0 = \ker \Phi $ e che $ M_{\mathscr{B^{*}}}^{\mathscr{B}} (\Phi) = M_{\mathscr{B}}(L_v) $ dove $ \mathscr{B}^{*} $ è la base del duale associata alla base $ \mathscr{B} $ di $ V $. 
\end{thm}

\begin{corollary}
	Un prodotto scalare $ \varphi $ è non degenere $ \iff $ $ V_0 = \{O_V\} $ $ \iff $ $ \ker M_{\mathscr{B}}(\varphi) = O_V $ $ \iff $ $ M_{\mathscr{B}}(\varphi) $ ha rango $ n = \dim V $. 
\end{corollary}

\begin{corollary}
	Se $ \mathscr{B} $ è una base ortonormale allora la matrice del prodotto scalare è diagonale. Detti $ \lambda_i = \varphi(v_i, v_i) $ si ha 
	\[M_{\mathscr{B}}(\varphi) = 
	\begin{pmatrix}
	\lambda_1 & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \ldots & \lambda_n \\
	\end{pmatrix}\]
\end{corollary}

\begin{prop}
	Se $ \mathscr{B} = \{v_1, \ldots, v_n\} $ è una base ortonormale di $ V $ allora \[n_0 (\varphi) = \mathrm{card}\{i : \lambda_i = \varphi(v_i, v_i) = 0\} = \dim V_0.\] Tale valore viene detto indice di nullità del prodotto scalare. 
\end{prop}

\begin{thm}[di Sylvester]
	\letvs \, $ \varphi $ un prodotto scalare su $ V $ e $ \mathscr{B} = \{v_1, \ldots, v_n\} $ una base di $ V $. Esiste un numero intero $ r = n_{+} (\varphi) $ che dipende solo da $ \varphi $ e non dalla base $ \mathscr{B} $, detto indice di positività, tale che ci sono esattamente $ r $ indici $ i $ tali che $ \varphi(v_i, v_i) = 1 $. Analogamente esiste un $ r' = n_{-} (\varphi) $ che dipende solo da $ \varphi $ e non dalla base $ \mathscr{B} $, detto indice di negatività, tale che ci sono esattamente $ r' $ indici $ i $ tali che $ \varphi(v_i, v_i) = - 1 $.
\end{thm}

\begin{definition}[segnatura]
	Si definisce segnatura di un prodotto scalare $ \varphi $ su $ V $ la terna \linebreak $ (n_0 (\varphi), n_{+} (\varphi), n_{-} (\varphi)) $. Detta $ n = \dim V $ vale \[n_0 (\varphi) + n_{+} (\varphi) + n_{-} (\varphi) = n.\] Dato inoltre $ W $ sottospazio vettoriale di $ V $ valgono le seguenti caratterizzazioni
	\begin{itemize}
		\item $ n_0 (\varphi) = \dim V_0 $ (radicale);
		\item $ n_{+} (\varphi) = \max{\{\dim W : W \subseteq V \text{ e } \varphi|_{W} > 0\}} $;
		\item $ n_{-} (\varphi) = \max{\{\dim W : W \subseteq V \text{ e } \varphi|_{W} < 0\}} $.
	\end{itemize}
\end{definition}

\clearpage

\section{Determinante}

\begin{definition}[gruppo simmetrico]
	Il gruppo simmetrico di un insieme è il gruppo formato dall'insieme delle permutazioni dei suoi elementi, cioè dall'insieme delle funzioni biiettive di tale insieme in se stesso, munito dell'operazione binaria di composizione di funzioni. \\
	In particolare detto $ S_n = \{1, \ldots, n\} $ l'insieme delle permutazioni \[\Sigma_n = \Sigma (S_n) = \{\sigma \colon S_n \to S_n : \sigma \text{ è biettiva}\}\] è un gruppo simmetrico. Ricordiamo che una permutazione $ \sigma \in \Sigma_n $ viene spesso indicata con la seguente notazione 
	\[\begin{pmatrix}
	1 & 2 & \cdots & n \\
	\sigma (1) & \sigma (2) & \cdots & \sigma (n) \\
	\end{pmatrix}\]
	dove si intende che l'elemento $ i $ viene mandato in $ \sigma (i) $ dalla permutazione. 
\end{definition}

\begin{definition}[trasposizione]
	Una trasposizione è una permutazione $ \tau \in \Sigma_n $ tale che scambia due soli elementi di $ S_n $ mentre lascia invariati i restaniti $ n - 2 $. Se $ \tau $ scambia $ i, j \in S_n $ scriveremo $ \begin{pmatrix}
	i & j \\
	\end{pmatrix} $. 
\end{definition}

\begin{prop}
	\begin{enumerate}[label = (\roman*)]
		\item Ogni permutazione $ \sigma \in \Sigma_n $ è esprimile, non in modo unico, come prodotto (composizione) di trasposizioni. 
		\item Se $ \sigma = \tau_1 \circ \cdots \circ \tau_h = \lambda_1 \circ \cdots \circ \lambda_h $ con $ \tau_i $ e $ \lambda_j $ trasposizioni allora $ h $ e $ k $ hanno la stessa parità. Se $ \sigma $ è prodotto di un numero pari (dispari) di trasposizioni diremo che $ \sigma $ è pari (dispari). 
	\end{enumerate}
\end{prop}

\begin{definition}
	Data $ \sigma \in \Sigma_n $ definiamo la funzione segno $ \sgn \colon \Sigma_n \to \{-1, 1\} $ come 
	\[\sgn(\sigma) = 
	\begin{cases*}
	1 & se $ \sigma $ è pari \\
	-1 & se $ \sigma $ è dispari \\
	\end{cases*}\]
	Vale in particolare che $ \sgn(\sigma_1 \circ \sigma_2) = \sgn(\sigma_1) \cdot \sgn(\sigma_2) $
\end{definition}

\begin{prop}
	Sia $ \sigma \in \Sigma_n $ una permutazione e sia $ \sigma^{-1} $ la permutazione inversa. Allora $ \sgn(\sigma) = \sgn(\sigma^{-1}) $. 
\end{prop}

\begin{thm}[Unicità del determinante]
	Sia $ \mathrm{Mat}_{n \times n} (\K) $  lo spazio vettoriale delle matrici quadrate  a valori nel campo $ \K $. Esiste una ed una sola funzione da $ \mathrm{Mat}_{n \times n} (\K) $ in $ \K $ funzione delle righe (o delle colonne) di una matrice $ A $ che rispetta i seguenti tre assiomi:  
	\begin{enumerate}[label = (\roman*)]
		\item \emph{multilineare} (lineare in ogni riga o colonna);
		\item \emph{alternante} (cambia di segno se si scambiano due righe o due colonne);
		\item \emph{normalizzata} (l'immagine dell'identità è 1);
	\end{enumerate}
	Tale funzione viene detta determinante ed indicata con $ \det \colon \mathrm{Mat}_{n \times n} (\K) \to \K $. 
\end{thm}

\begin{propriety}[del determinante] \label{prop:det}
	Le seguenti proprietà sono conseguenza degli assiomi (i), (ii) e (iii). Sia $ A \in \mathrm{Mat}_{n \times n} (\K) $ allora
	\begin{enumerate}[label = (\arabic*)]
		\item Se $ A $ ha due righe uguali allora $ \det{A} = 0 $.
		\item Se $ A $ ha una riga nulla allora $ \det{A} = 0 $.
		\item Se alla riga $ A_i $ di $ A $ si somma un multiplo della riga $ A_j $ ($ i \neq j $) si ottiene una matrice $ B $ tale che $ \det A = \det B $. 
		\item Il determinante è invariante sotto l'algoritmo di Gauss (escludendo le mosse di \emph{normalizzazione} delle righe o delle colonne) a meno di un segno che dipende dal numero di scambi di righe o di colonne fatto. In altre parole se $ S $ è una forma a scalini di $ A $ allora $ \det A = \pm \det S $.
		\item Se $ A $ è una matrice diagonale allora il suo determinante è il prodotto degli elementi sulla diagonale: $ \det A = a_{11} \cdots a_{nn} $. 
	\end{enumerate}
\end{propriety}

\begin{thm}[esistenza del determinante]
	Sia $ A = (a_{ij})_{\substack{i = 1, \ldots, n \\ j = 1, \ldots, n}} \in \mathrm{Mat}_{n \times n} (\K) $. La funzione \[\det(A) = \sum_{\sigma \in \Sigma_n} \sgn(\sigma) \cdot a_{1 \sigma(1)} a_{2 \sigma(2)} \cdots a_{n \sigma{n}}\] è il determinante (in quanto è una funzione multilineare, alternante e normalizzata dallo spazio delle matrici nel campo). 
\end{thm}

\begin{corollary}
	Il determinante di $ A $ è uguale al determinante della sua trasposta: $ \det A = \det A^{t} $. 
\end{corollary}

\begin{definition}[complemento algebrico]
	Il complemento algebrico o cofattore dell'elemento $ a_{ij} $ di una matrice $ A \in \mathrm{Mat}_{n \times n} (\K) $ è il determinante della matrice $ (n - 1) \times (n - 1) $ ottenuta cancellando da $ A $ la $ i $-esima riga e la $ j $-esima colonna moltiplicato per $ (-1)^{i + j} $: in formule
	\[\cof_{ij}(A) = (-1)^{i + j} \cdot \det 
	\begin{pmatrix}
		a_{11} & \cdots & \cancel{a_{1j}} & \cdots & a_{1n} \\
		\vdots &        & \vdots &        & \vdots \\
		\cancel{a_{i1}} & \cdots & \cancel{a_{ij}} & \cdots & \cancel{a_{in}} \\
		\vdots &  		& \vdots &  	  & \vdots \\
		a_{n1} & \cdots & \cancel{a_{nj}} & \cdots & a_{nn} \\
	\end{pmatrix}\]
\end{definition}

\begin{thm}[sviluppo di Laplace]
	Data $ A \in \mathrm{Mat}_{n \times n} (\K) $ la seguente funzione
	\begin{itemize}
		\item fissata una riga $ i $ di $ A $: $ \det A = \sum_{j = 1}^{n} a_{ij} \cdot \cof_{ij} (A) $
		\item fissata una colonna $ j $ di $ A $: $ \det A = \sum_{i = 1}^{n} a_{ij} \cdot \cof_{ij} (A) $
	\end{itemize}
	verifica gli assiomi (i), (ii) e (iii) e quindi è il determinante. \\
	Dallo sviluppo di Laplace si deduce che la proprietà (5) di Proprietà \ref{prop:det} vale anche per le matrici triangolari (superiori o inferiori)
\end{thm}

\begin{thm}[invertibilità]
	$ A $ è invertibile $ \iff $ $ \det{A} \neq 0 $ ($ \iff $ $ \rg A = n $).
\end{thm}

\begin{prop}[formula per l'inversa]
Sia $ A \in \mathrm{Mat}_{n \times n} (\K) $ invertibile, i.e. $ \det A \neq 0 $. Allora il coefficiente $ ij $ della matrice inversa è \[\left(A^{-1}\right)_{ij} = \frac{1}{\det{A}} \cdot \cof_{ji}{(A)}\] dove $ \cof_{ji}{(A)} $ è il complemento algebrico dell'elemento $ a_{ji} $ di $ A $ (sì, gli indici sono scambiati).   
\end{prop}

\begin{thm}[regola di Cramer]
	Sia $ A \in \mathrm{Mat}_{n \times n} (\K) $ invertibile, i.e $ \det A \neq 0 $ (e $ \rg A = n $), e siano $ A^{1}, \ldots, A^{n} $ le sue colonne. Siano inoltre $ b = (b_j) $ un vettori colonna. Allora se $ x = (x_j) $ è l'unico vettore colonna che soddisfa il sistema lineare 
	\[Ax = b \quad \iff \quad A \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} = \begin{pmatrix} b_1 \\ \vdots \\ b_n \end{pmatrix} \quad \iff \quad x_1 A^{1} + \ldots x_n A^{n} = b\]
	ha componenti date da 
	\[x_j = \frac{\det{\left(A^{1} \cdots A^{j - 1} \ b \ A^{j + 1} \ldots A^{n}\right)}}{\det{A}}\]
	dove per $ A^{1} \cdots A^{j - 1} \ b \ A^{j + 1} \ldots A^{n} $ si intende la matrice $ A $ alla cui $ j $-esima colonna è stato sostituito il vettore colonna $ b $ dei termini noti.  
\end{thm}

\begin{corollary}
	Sia $ A \in \mathrm{Mat}_{n \times n} (\K) $ e $ \cof{A} $ la matrice dei cofattori. Allora vale la seguente identità \[A (\cof{A})^{t} = \det{(A)} \cdot \Id\]
\end{corollary}

\begin{thm}[di Binet]
	Date $ A, B \in \mathrm{Mat}_{n \times n} (\K) $ vale $ \det{(AB)} = \det{(A)} \cdot \det{(B)} $. 
\end{thm}

\begin{corollary}[determinante dell'inversa]
	Se $ A \in \mathrm{Mat}_{n \times n} (\K) $ è invertivile, i.e. $ \det{A} \neq 0 $, allora $ \det{A^{-1}} = \frac{1}{\det{A}} $. 
\end{corollary}

\begin{corollary}
	Per ogni $ A, B \in \mathrm{Mat}_{n \times n} (\K) $ vale $ \det{AB} = \det{BA} $.
\end{corollary}

\begin{corollary}[invarianza del determinatane per coniugio]
	Siano $ A, B \in \mathrm{Mat}_{n \times n} (\K) $ con $ B $ matrice invertibile. Allora $ \det{(B^{-1} A B)} = \det{(A)} $. \\
	In altri termini se $ [A] $ e $ [A'] $ sono matrici che descrivono lo stesso endomorfismo $ A \colon V \to V $ ma scritte in basi (in partenza ed in arrivo) diverse allora $ \det{[A]} = \det{[A']} $. Dunque è il determinante è ben definito come funzione dagli endomorfismi $ \mathscr{L}(V) $ nel campo $ \K $ . 
\end{corollary}

\begin{definition}[sottomatrice]
	Sia $ M \in \mathrm{Mat}_{n \times m} (\K) $ una matrice qualsiasi. Per sottomatrice di $ M $ si intende una ottenuta da $ M $ cancellando alcune righe e/o alcune colonne di $ M $. In modo equivalente si intende una $ M' \in \mathrm{Mat}_{r \times s} (\K) $ ottenuta da $ M $ selezionando i coefficienti posti nell'intersezione tra $ 1 \leq r \leq n $ righe ed $ 1 \leq s \leq m $ colonne scelte nella matrice $ M $. Nel caso di sottomatrice quadrate si ha $ r = s = k $ e si dice che l'ordine di $ M' $ è $ k $. 
\end{definition}

\begin{prop}
	Se $ v_1 = \begin{pmatrix} v_{11} \\ v_{n1} \end{pmatrix}, \ldots, v_k = \begin{pmatrix} v_{1k} \\ v_{nk} \end{pmatrix} $ con $ k \leq n $ sono vettori linearmente \emph{dipendenti} allora ogni sottomatrice quadrata di ordine $ k $ estratta dalla matrice $ M \in \mathrm{Mat}_{n \times k} (\K) $ che ha per colonne $ v_1, \ldots, v_k $ non invertibile e quindi con determinante nullo. 
	\[M = \begin{pmatrix}[c|c|c]
	v_{11} & & v_{1k} \\
	\vdots & \cdots & \vdots \\
	v_{n1} & & v_{nk}
	\end{pmatrix}\]
\end{prop}

\begin{prop}
	Se $ v_1 = \begin{pmatrix} v_{11} \\ v_{1n} \end{pmatrix}, \ldots, v_k = \begin{pmatrix} v_{k1} \\ v_{kn} \end{pmatrix} $ con $ k \leq n $ sono vettori linearmente \emph{indipendenti} allora esiste una sottomatrice quadrata di ordine $ k $ estratta dalla matrice $ M \in \mathrm{Mat}_{n \times k} (\K) $ che ha per colonne $ v_1, \ldots, v_k $ invertibile e quindi con determinante non nullo. 
	\[M = \begin{pmatrix}[c|c|c]
	v_{11} & & v_{1k} \\
	\vdots & \cdots & \vdots \\
	v_{n1} & & v_{nk}
	\end{pmatrix}\]
\end{prop}

\begin{thm}[caratterizzazione del rango con il determinante]
	Sia $ A \in \mathrm{Mat}_{n \times n} (\K) $. Allora il rango di $ A $ è il massimo ordine di una sottomatrice quadrata invertibile, i.e con determinante non nullo. \\
	In altri termini $ \rg A = k $ $ \iff $ tra tutte le sottomatrici di $ A $ esiste una sottomatrice $ k \times k $ con determinante $ \neq 0 $ tale che tutte le sottomatrici quadrate di ordine maggiore hanno determinante nullo. Per lo sviluppo di Laplace è sufficiente verificare che tutte le sottomatrici $ (k + 1) \times (k + 1) $ hanno determinante nullo. 
\end{thm}

\begin{propriety}[del determinante]
	Valgono le seguenti identità aggiuntive. 
	\begin{enumerate}
		\item Se $ M = \begin{pmatrix}[c|c]
		A & B \\ \hline
		C & D
		\end{pmatrix} $	
		è una matrice a blocchi con $ A $ matrice invertibile allora $ \det{M} = \det{(A)} \cdot \det{(D - CA^{-1}B)} $. Inoltre che $ AC = CA $ allora $ \det{M} = \det{AD - CB} $. 
		\item Se $ M $ è una matrice diagonale a blocchi allora il determinante è il prodotto dei determinanti dei blocchi diagonali $ A_1, \ldots A_k $. 
		\[\det{M} = \det
		\begin{pmatrix}
		A_1		&		&		&		&		 \\
				& A_2	&		&		&		 \\
				& 		&\ddots &		&		 \\	 
				& 		&		& A_{k - 1}		&		 \\	
				& 		& &		& A_{k}		 \\	 		 		
		\end{pmatrix}
		= \det{(A_1)} \cdots \det{(A_k)}\]
		\item Se $ M $ è una \emph{matrice di Vandermonde} allora vale che 
		\[\det{(M)} = \det 
		\begin{pmatrix}
			1 & 1 & \cdots & 1 \\
			a_1 & a_2 & \cdots & a_n \\
			a_1^{2} & a_2^{2} & \cdots & a_n^{2} \\
			\vdots & \vdots & & \vdots \\
			a_1^{n} & a_2^{n} & \cdots & a_n^{n} \\
		\end{pmatrix} 
		= \prod_{\substack{i = 1 \\ j < i}}^{n} (a_i - a_j)\]
	\end{enumerate}
\end{propriety}

\clearpage

\section{Diagonalizzazione, autovettori e autovalori}

\begin{definition}[autovettore, autovalore]
	\letvs. Sia $ T \colon V \to V $ un endomorfismo su $ V $. $ V \in V \setminus \{O_V\} $ si dice autovettore di $ T $ se esiste $ \lambda \in \K $ tale che \[T(v) = \lambda v.\] Si dice in questo caso che $ \lambda $ è autovalore per $ T $ (relativo a $ v $). \\
	Notiamo che tutti i $ v \in \ker V \setminus \{O_V\} $ sono autovettori per $ v $ con autovalore 0.  
\end{definition}

\begin{definition}[autospazio]
	Dato $ \lambda \in \K $ chiamiamo $ V_\lambda = \{v \in V : T(v) = \lambda v\} $ l'autospazio relativo a $ \lambda $. Segue dalla definizione che $ V_\lambda = \ker \left(T - \lambda \cdot \Id\right) $
\end{definition}

\begin{prop}
	Se $ v_1, \ldots, v_n \in V $ sono autovettori per $ T $ con relativi autovalori $ \lambda_1, \ldots, \lambda_n \in \K $ e $ \mathscr{B} = \{v_1, \ldots, v_n\} $ è una base di $ V $ allora la matrice di $ T $ rispetto a $ \mathscr{B} $ (sia in partenza che in arrivo) è diagonale. 
	\[[T]_{\mathscr{B}}^{\mathscr{B}} = 
	\begin{pmatrix}
	\lambda_1 & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \ldots & \lambda_n \\
	\end{pmatrix}\]
\end{prop}

\begin{definition}[polinomio caratteristico]
	Sia $ T \colon V \to V $ un endomorfismo. Fissata una base $ \mathscr{B} = \{v_1, \ldots, v_n\} $ di $ V $ chiamiamo \[p_T(t) = \det \left(t \cdot \Id - [T]_{\mathscr{B}}^{\mathscr{B}}\right)\] il polinomio caratteristico di $ T $. 
\end{definition}

\begin{prop}
	Il polinomio caratteristico non dipende dalla base scelta. In altri termini se $ \mathscr{B} = \{v_1, \ldots, v_n\} $ e $ \mathscr{B}' = \{v'_1, \ldots, v'_n\} $ sono basi di $ V $ allora \[p_T(t) = \det \left(t \cdot \Id - [T]_{\mathscr{B}}^{\mathscr{B}}\right) = \det \left(t \cdot \Id - [T]_{\mathscr{B}'}^{\mathscr{B}'}\right) = \det \left(t \cdot \Id - T\right)\]
\end{prop}

\begin{thm}
	Sia $ T \colon V \to V $ un endomorfismo. Allora $ \lambda \in \K $ è un autovalore di $ T $ se e solo se $ \lambda $ è radice di $ p_T(t) $, ossia $ p_T(\lambda) = 0 $. 
\end{thm}

\begin{thm}
	Dati $ \lambda_1, \ldots, \lambda_k $ autovalori di $ T $ a due a due distinti, siano $ v_1, \ldots, v_k $ gli autovettori corrispondenti: $ T(v_1) = \lambda_1 v_1, \ldots, T(v_k) = \lambda_k v_k $. Allora $ v_1, \ldots, v_k $ sono linearmente indipendenti. 
\end{thm}

\begin{thm}[somma diretta degli autospazi]
	Sia $ T \colon V \to V $ un endomorfismo e $ \lambda_1, \ldots, \lambda_k $ autovalori di $ T $ a due a due distinti. Allora gli autospazi $ V_{\lambda_1}, \ldots, V_{\lambda_k} $ sono in somma diretta. \\
	Più in generale se $ A_1, \ldots, A_k $ sottospazi di $ V $ sono tali che per ogni insieme $ \{v_1, \ldots, v_k\} $ di vettori non nulli linearmente indipendenti tali che $ v_i \in A_i $, allora $ A_1, \ldots, A_k $ sono in somma diretta. 
\end{thm}

\begin{definition}[molteplicità algebrica e geometrica]
	Sia $ T \colon V \to V $ un endomorfismo e sia \[p_T(t) = (t - \lambda_1)^{a_1} \cdots (t - \lambda_k)^{a_k} \cdot f(t)\] il polinomio caratteristico dove $ \lambda_1, \ldots, \lambda_k $ sono le radici del polinomio, i.e autovalori di $ T $, e $ f(t) $ un polinomio irriducibile in $ \K $. Diremo che $ a_i $ è la molteplicità algebrica dell'autovalore $ \lambda_i $. Diremo inoltre che $ m_i = \dim V_{\lambda_i} $ è la molteplicità geometrica di $ \lambda_i $. \\
	Notiamo che se $ T $ è diagonalizzabile allora $ f(t) = 1 $. 
\end{definition}

\begin{thm}
	Sia $ T \colon V \to V $ un endomorfismo e $ \lambda_1, \ldots, \lambda_k $ con $ k \leq n $ autovalori. Allora $ \forall i = 1, \ldots, k $ vale $ 1 \leq m_i \leq a_i $ (molteplicità geometrica $ \leq $ molteplicità algebrica). 
\end{thm}

\begin{corollary}[criterio sufficiente per la diagonalizzazione]
	Sia $ T \colon V \to V $ un endomorfismo e $ p_T(t) $ il suo polinomio caratteristico. Se $ p_T(t) $ ha tutte le radici in $ \K $ a due a due distinte allora $ T $ è diagonalizzabile. 
\end{corollary}

\begin{thm}
	Sia $ T \colon V \to V $ un endomorfismo. Allora $ T $ è diagonalizzabile se e solo se $ f(t) = 1 $ (il polinomio si fattorizza completamente nel campo) e $ \forall \lambda_i $ autovalore $ m_i = a_i $. 
\end{thm}

\begin{definition}[polinomio minimo]
	Sia $ T \colon V \to V $ un endomorfismo. Chiamiamo polinomio minimo di $ T $ il polinomio di grado più piccolo (\emph{wlog} monico) $ \mu_T(t) \in \K[t] $ tale che \[\mu_T(T) = T^j + \ldots + b_1 T + b_0 \Id = 0.\] 
\end{definition}

\begin{thm}
	Sia $ T \colon V \to V $ un endomorfismo. Se $ h(t) \in \K[t] $ soddisfa la proprietà $ h(T) = 0 $ allora $ \mu_T(t) $ divide $ h(t) $. 
\end{thm}

\begin{thm}[di Hamilton - Cayley] \label{thm:HC}
	Dato $ T \colon V \to V $ endomorfismo vale che $ p_T(T) = 0 $ e quindi che il polinomio minimo divide il polinomio caratteristico. 
\end{thm}

\begin{prop}
	Sia $ A, B \in \mathrm{Mat}_{n \times n} (\K) $ con $ B $ matrice quadrata invertibile. Allora se $ q(t) = c_n t^{n} + \ldots + c_1 t + c_0 $ un polinomio vale \[q(B^{-1} A B) = B^{-1} p(A) B. \] Se $ T \colon V \to V $ è un endomorfismo  e $ q(t) = p_T(t) $ è il polinomio caratteristico di $ T $ (o un qualsiasi polinomio tale che $ q(T) = 0 $) si ha che $ p_T(T) = 0 \Leftrightarrow p_T(B^{-1} T B) = 0 $ e quindi il polinomio minimo non dipende dalla base e l'enunciato del Teorema \ref{thm:HC} non dipende dalla base scelta per $ V $. 
\end{prop}

\begin{prop}
	$ T \colon V \to V $ endomorfismo è diagonalizzabile se e solo se le radici del polinomio minimo $ \mu_T(t) $ hanno molteplicità algebrica 1, ovvero $ \mu_T(t) $ non ha radici doppie. 
\end{prop}

\begin{prop}
	Sia $ T \colon V \to V $ un endomorfismo, $ p_T(t) $ il polinomio caratteristico e $ \mu_T(t) $ il polinomio minimo. Allora $ p_T(\lambda) = 0 \Rightarrow \mu_T(\lambda) = 0 $. 
\end{prop}

\begin{thm}[triangolazione]
	Data una qualunque matrice $ M \in \mathrm{Mat}_{n \times n} (\C) $ esiste una matrice $ C $ invertibile (matrice di cambio base) tale che $ CMC^{-1} $ è triangolare superiore. L'enunciato vale in generale su $ \mathrm{Mat}_{n \times n} (\K) $ quando il polinomio caratteristico ha tutte le radici nel campo. \\ Moralmente ogni matrice è triangolarizzabile. 
\end{thm}

\vspace{2mm}

\begin{framed}
	\begin{center}
		\textbf{Diagonalizzazione - una strategia in 4 passi}
	\end{center}
	\begin{enumerate}
		\item Calcolare il polinomio caratteristico e trovarne le radici. Se il polinomio minimo non si fattorizza completamente nel campo, $ T $ non è diagonalizzabile. 
		\item Detti $ \lambda_1, \ldots, \lambda_k $ con $ k \leq n $ calcolo $ V_{\lambda_1} = \ker(T - \lambda_1 \Id), \ldots, V_{\lambda_k} = \ker(T - \lambda_k \Id) $
		\item Osservo che questi sottospazi sono in somma diretta e quindi 
			\begin{itemize}
				\item se $ V_{\lambda_1} \oplus \ldots \oplus V_{\lambda_k} = V $ allora $ T $ si diagonalizza e una base per $ V $ è data dall'unione delle basi dei $ V_{\lambda_j} $;
				\item se $ V_{\lambda_1} \oplus \ldots \oplus V_{\lambda_k} \subset V $ allora $ T $ non è diagonalizzabile.
			\end{itemize} 
		Se voglio solo sapere se $ T $ è diagonalizzabile è sufficiente confrontare la molteplicità algebrica $ a_i $ delle radici del polinomio caratteristico $ \lambda_i $ con la molteplicità geometrica $ m_i = \dim V_{\lambda_i} = \dim \ker (T - \lambda_i \Id) $. Se per ogni $ i $ vale $ a_i = m_i $ allora $ T $ è diagonalizzabile, altrimenti non lo è. 
		\item Usando la base trovata, se $ T $ è diagonalizzabile, si scrive la matrice diagonale corrispondente (i coefficienti sono zeri tranne sulla diagonale in cui ci sono tanti $ \lambda_i $ tanti quanti la $ m_i = \dim V_{\lambda_i} $). 
	\end{enumerate} 
\end{framed}

\end{document}
