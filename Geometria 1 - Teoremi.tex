\documentclass[9pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}

\usepackage[a4paper,top=3.cm,bottom=3.cm,left=3cm,right=3cm]{geometry}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{cancel}

\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\letvs}{Sia $ V $ uno spazio vettoriale sul campo $ \K $}
\newcommand{\letvsR}{Sia $ V $ uno spazio vettoriale sul campo $ \R $}
\newcommand{\letvsC}{Sia $ V $ uno spazio vettoriale sul campo $ \C $}
\newcommand{\letlin}{Sia $ L \colon \K^n \to \K^n $ un'applicazione lineare}
\newcommand{\Ker}{\mathrm{Ker}\,}
\newcommand{\Imm}{\mathrm{Im}\,}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\rg}{\mathrm{rg}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\cof}{\mathrm{cof}}
\newcommand{\scprd}[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newtheoremstyle{mythm}{\topsep}{\topsep}{\rmfamily}{}{\bfseries}{}{.5em}{}

\theoremstyle{mythm}
\newtheorem{axiom}{Assioma}[section]
\newtheorem{definition}{Definizione}[section]
\newtheorem{propriety}{Proprietà}[section]
\newtheorem{thm}{Teorema}[section]
\newtheorem{corollary}[thm]{Corollario}
\newtheorem{prop}[thm]{Proposizione}

\title{\textsc{Geometria 1}}
\author{Alessandro Piazza \thanks{alessandro.piazza@sns.it}}


\begin{document}

\maketitle

\begin{abstract}
	Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
\end{abstract}

\tableofcontents

\clearpage

\section{Spazi Vettoriali}
\begin{definition}[campo]
	Un campo $ \K $ è un insieme su cui sono definite due operazioni di \emph{somma} e $ + $ e \emph{prodotto} $ \cdot $ che associano a due elementi dell'insieme un altro elemento dell'insieme
	\begin{enumerate}
		\item $ \forall x, y \in \K \Rightarrow x + y \in \K $
		\item $ \forall x, y \in \K \Rightarrow x \cdot y \in \K $
	\end{enumerate}
	e che rispettano le seguenti proprietà
	\begin{enumerate}[label=(\roman*)]
		\item \emph{associativa}
		\item \emph{commutativa}
		\item \emph{esistenza degli elementi neutri}
		\item \emph{opposto}
		\item \emph{inverso}
		\item \emph{distributiva del prodotto rispetto alla sommma}
	\end{enumerate}
\end{definition}

\begin{definition}[spazio vettoriale]
	Uno spazio vettoriale su un campo $ \K $ è un insieme  $ V $ su cui sono definite due operazioni di 
	\begin{enumerate}
		\item \emph{somma} tra elementi di $ V $: $ \forall v, w \in V \Rightarrow v + w \in V $
		\item \emph{prodotto per scalare}: $ \forall v \in V, \forall \lambda \in \K \Rightarrow\lambda v \in V $
	\end{enumerate}
	che devono rispettare le seguenti proprietà:
	\begin{enumerate}[label=(\roman*)]
		\item \emph{associativa della somma}
		\item \emph{commutativa della somma}
		\item \emph{esistenza dell'elemento neutro della somma}
		\item \emph{opposto della somma}
		\item \emph{distributiva del prodotto rispetto alla somma}
		\item \emph{associativa del prodotto}
		\item \emph{elemento neutro del prodotto}
	\end{enumerate}
	Chiameremo \emph{vettori} gli elementi di $ V $.
\end{definition}

\begin{propriety} Uno spazio vettoriale gode delle seguenti proprietà:
	\begin{enumerate}
		\item Unicità dell'elemento neutro
		\item Unicità dell'opposto
		\item $ 0 \cdot v = O_V $
		\item $ (-1) \cdot v = -v $
	\end{enumerate}
\end{propriety}

\begin{definition}[sottospazio vettoriale]
	\letvs. Diciamo che $ W \subseteq V $ è un sottospazio vettoriale di $ V $ se valgono le seguenti proprietà
	\begin{enumerate}[label=(\roman*)]
		\item $ \forall v, w \in W \Rightarrow v + w \in W $
		\item $ \forall v \in W, \forall \lambda \in \K \Rightarrow \lambda v \in W $
		\item $ O_V \in W $
	\end{enumerate}
\end{definition}

\clearpage

\begin{thm}[intersezione di sottospazi]
	\textsf{Intersezione di sottospazi è un sottospazio vettoriale}
\end{thm}

\begin{thm}[somma di sottospazi]
	\textsf{Somma di sottospazi è un sottospazio vettoriale}
\end{thm}

\begin{definition}[combinazione lineare]
	\letvs, siano $ v_1, \ldots , v_n \in \nobreak V $ e $ \lambda_1, \ldots , \lambda_n \in \K $. Si dice combinazione lineare dei $ v_i $ un vettore $ v \in V $ tale che \[v = \sum_{i=1}^{n} \lambda_i v_i = \lambda_1 v_1 + \ldots + \lambda_n v_n \]
\end{definition}

\begin{definition}[span]
	\[Span\{v_1, \ldots , v_n\} = \left\{ v \in V: \exists \lambda_1, \ldots , \lambda_n \in \K: v = \sum_{i=1}^{n} \lambda_i v_i \right\}\]
\end{definition}

\begin{thm}[proprietà dello span]
	\letvs \, e siano $ v_1, \ldots , v_n \in \nolinebreak V $. Allora $ Span\{v_1, \ldots , v_n\} $ è il più piccolo sottospazio vettoriale di $ V $ che contiene tutti i $ v_i $.
\end{thm}

\begin{definition}[dipendenza e indpendenza lineare] 
	\letvs \, e siano $ v_1, \ldots , v_n \in V $. Si dice che $ v_1, \ldots, v_n $ sono linearmente dipendenti se $ \exists \lambda_1, \ldots , \lambda_n \in \K $ non tutti nulli tali che \[\lambda_1 v_1 + \ldots + \lambda_n v_n = O_V.\] Analogamente si dice che $ v_1, \ldots, v_n $ sono linearmente indipendenti se \[\lambda_1 v_1 + \ldots + \lambda_n v_n = O_V \Leftrightarrow \lambda_1 = \ldots = \lambda_n = 0.\]	
\end{definition}

\begin{definition}[base]
	\letvs. Un insieme $ \{v_1, \ldots, v_n\} $ si dice base di $ V $ se
	\begin{enumerate}[label=(\roman*)]
		\item $ v_1, \ldots, v_n $ sono linearmente indipendenti
		\item $ Span\{v_1, \ldots , v_n\} = V $
	\end{enumerate}
\end{definition}

\begin{thm}[unicità della combinazione lineare]
	contenuto...
\end{thm}

\begin{definition}[sottoinsieme massimale]
	\textsf{se aggiungo un vettore l'insieme non è più linearmente indipendente}	
\end{definition}

\begin{thm}
	\textsf{linearmente indipendente + massimale = base}
\end{thm}

\begin{thm}
	\letvs \ e sia $ \{v_1, \ldots, v_m\} $ una base di $ V $. Se $ w_1, \ldots, w_n $, con $ n > m $, sono vettori di $ V $, allora $ w_1, \ldots, w_n $ sono linermente indipendenti.
\end{thm}

\begin{corollary}
	\letvs. Supponiamo di avere due basi di $ V $, una con $ n $ elementi e una con $ m $ elementi. Allora $ n = m $.
\end{corollary}

\begin{definition}[dimensione]
	\letvs \, avente una base costituita da $ n $ vettori. Allora diremo che $ V $ ha dimensione $ n $ e scriveremo $ \dim V = n $.
\end{definition}

\begin{thm}
	\textsf{sono n + generano = base}
\end{thm}

\begin{thm}
	\textsf{sono n + set massimale = base}
\end{thm}

\begin{thm}
	\textsf{sono n + linearmente indipendenti = base}
\end{thm}

\begin{thm}[completamento ad una base]
	\letvs \, di dimensione $ n \geq 2 $. Sia $ r $ un intero positivo con $ 0 < r < n $. Dati $ r $ vettori $ v_1, \ldots , v_r \in V $ linearmente indipendenti è possibile completarli ad una base di $ V $, ossia trovare vettori $ v_{r+1}, \ldots, v_n $ tali che $ \{v_1, \ldots , v_r, v_{r+1}, \ldots , v_n\} $ è base di $ V $.
\end{thm}

\clearpage

\section{Matrici}

\begin{definition}[matrice]
	Una matrice $ m \times n $ a coefficienti in $ \K $ è un tabella ordinata di $ m $ righe e $ n $ colonne i cui elementi appartengono ad un campo $ \K $. Dati $ a_{ij} \in \K $ con $ i = 1, \ldots, m $ e $ j = 1, \ldots, n $ diremo che $ A \in \mathrm{Mat}_{m \times n} (\K) $ e scriveremo
	\[ A = 
	\begin{pmatrix}
		a_{11} & \cdots  & a_{1n} \\
		\vdots & \ddots & \vdots \\
		a_{m1} & \cdots  & a_{mn} \\
	\end{pmatrix}\]
\end{definition}

\begin{definition}[matrice diagonale e identità]
	contenuto...
\end{definition}

\begin{definition}[prodotto tra matrici]
	contenuto...
\end{definition}

\begin{propriety}
	\textsf{proprietà del prodotto ($ n \times n $ stabile rispetto al prodotto)}
\end{propriety}

\begin{definition}[matrice trasposta]
	contenuto...
\end{definition}

\begin{propriety}[della trasposta]
	contenuto...
\end{propriety}

\begin{definition}[matrici coniugate]
	Due matrici $ A $ e $ B $ si dicono coinugate se esiste una matrice $ P $ invertibile tale che \[B = P^{-1} A P.\] Matrici coniugate rappresentano la stessa applicazioni lineari viste in due basi diverse. 
\end{definition}

\begin{definition}[traccia]
	Sia $ M $ una matrice quadrata $ n \times n $. La traccia di $ M $ è la somma degli elementi sulla diagonale \[ \tr(M) = \tr 
	\begin{pmatrix}
	a_{11} & \cdots  & a_{1n} \\
	\vdots & \ddots & \vdots \\
	a_{n1} & \cdots  & a_{nn} \\
	\end{pmatrix}
	= a_{11} + \ldots + a_{nn}\]
\end{definition}

\begin{propriety}[della traccia]
	La traccia gode delle seguenti proprietà
	\begin{enumerate}[label=(\roman*)]
		\item $ \tr (A + B) = \tr(A) + \tr(B) $ e $ \tr(\lambda A) = \lambda \, \tr (A) $
		\item $ \tr(\prescript{t}{}{A}) = \tr (A) $
		\item $ \tr (AB) = \tr (BA) $
	\end{enumerate}
\end{propriety}

\begin{thm}[invarianza della traccia per coniugio]
	Se $ A $ e $ B $ sono matrici coniugate, allora $ \tr (A) = \tr (B) $
\end{thm}

Roba su riduzione a scalini...

\clearpage

\section{Applicazioni lineari}

\begin{definition}[applicazione lineare]
	Siano $ V $ e $ W $ due spazi vettoriali su un campo $ \K $. Diremo che una funzione $ L \colon V \to W  $ è un'applicazione lineare (o mappa lineare) se $ L $ soddisfa le seguenti proprietà
	\begin{enumerate}[label=(\roman*)]
		\item $ \forall v_1, v_2 \in V $ vale $ L(v_1 + v_2) = L(v_1) + L(v_2) $
		\item $ \forall v \in V, \forall \lambda \in \K $ vale $ L(\lambda v) = \lambda L(v)$
	\end{enumerate}
	Diretta conseguenza è la seguente proprietà chiave delle applicazioni lineari
	\begin{enumerate}[resume, label=(\roman*)]
		\item $ L(O_V) =O_W $
	\end{enumerate}
\end{definition}

\begin{definition}[nucleo]
	Siano $ V $ e $ W $ due spazi vettoriali su un campo $ \K $ e sia $ L \colon V \to W  $ un'applicazione lineare. Definiamo nucleo o kernel di $ L $, e scriveremo $ \Ker L$, l'insieme degli elementi di $ V $ la cui immagine attraverso $ L $ è lo zero di $ W $. Formalmente \[\Ker L = \{v \in V \colon L(v) = O_W\} \]
\end{definition}

\begin{thm}
	Sia $ L \colon V \to W  $ un'applicazione lineare. Allora $ \Ker L $ è un sottospazio vettoriale di $ V $.
\end{thm}

\begin{thm}
	Sia $ L \colon V \to W  $ un'applicazione lineare. Allora $ \Imm L $ è un sottospazio vettoriale di $ W $.
\end{thm}

\begin{thm}
	Un'applicazione lineare $ L \colon V \to W $ è iniettiva se e solo se $ \Ker L = \{O_V\} $.
\end{thm}

\begin{thm}[composizione di applicazioni lineari]
	\textsf{la composizione di due applicazioni lineari è ancora un'applicazione lineare}
\end{thm}

\begin{thm}[inversa di un'applicazione lineare]
	\textsf{l'inversa di una applicazioni lineare (se esiste) è ancora un'applicazione lineare}
\end{thm}

\begin{thm}
	\letlin \, tale che $ \Ker L = \{O_V\} $. Se $ v_1, \ldots, v_n \in V $ sono vettori linearmente indipendenti, anche $ L(v_1), \ldots, L(v_n) $ sono vettori linearmente indipendenti di $ W $.
\end{thm}

\begin{thm}[delle dimensioni]
	Siano $ V $ e $ W $ spazi vettoriali su un campo $ \K $ e sia $ L \colon V \to W $  un'applicazione lineare. Allora vale \[\dim V = \dim \Imm L + \dim \Ker L\]
\end{thm}

\begin{thm}
	\letlin. Se $ \Ker L = \{O_V\} $ e $ \Imm L = W $, allora $ L $ è biettiva e dunque invertibile
\end{thm}

\begin{definition}[isomorfismo]
	Un'applicazione lineare $ L \colon V \to W $ biettiva si dice isomorfismo. Se tale applicazioni esiste si dice che $ V $ e $ W $ sono isomorfi.
\end{definition}

\begin{definition}[endomorfismo]
	Un'applicazione lineare $ L \colon V \to V $ dallo spazio in sé si dice endomorfismo.
\end{definition}

\clearpage

\section{Applicazioni lineari e matrici}

\begin{thm}
	Sia $ L \colon \K^n \to \K $ un'applicazione lineare. Allora esiste un unico vettore $ A \in \K^n $ tale che $ \forall X \in \K^n $ \[L(X) = A \cdot X\]
\end{thm}

\begin{thm}
	Esiste una corrispondenza biunivoca tra \[\{L \colon \K^n \to \K^m \quad \mathrm{lineare}\} \leftrightarrow \mathrm{Mat}_{m \times n}(\K)\]
	\begin{enumerate}
		\item Data una matrice $ M \in \mathrm{Mat}_{m \times n}(\K) $ è possibile associare ad essa un'applicazione lineare
		\begin{align*}
		L \colon \K^n & \to \K^m \\
		X & \mapsto MX
		\end{align*}
		\item Sia $ L \colon \K^n \to \K^m $ un'applicazione lineare. Allora esiste una matrice $ M \in \mathrm{Mat}_{m \times n}(\K) $ tale che $ \forall X \in \K^n $, $ L(X) = MX $. Se $ \mathscr{C} = \{e_1, \ldots , e_n\} $ la è base canonica di $ \K^n $, le colonne di $ M $ sono $ L(e_1), \ldots, L(e_n) $.
	\end{enumerate}
\end{thm}

\begin{thm}
	Siano $ V $ e $ W $ spazi vettoriali su un campo $ \K $ e sia $ L \colon \K^n \to \K^m $ un'applicazione lineare. Siano inoltre $ \mathscr{B} = \{v_1, \ldots, v_n\} $ e $ \mathscr{B}' = \{w_1, \ldots, w_m\} $ basi rispettivamente di $ V $ e di $ W $. Preso $ v \in V $ esiste una matrice $ M \in \mathrm{Mat}_{m \times n}(\K) $ tale che \[X_{\mathscr{B} '} (L(v)) = M X_{\mathscr{B}} (v) \] dove $ X(w) $ è il vettore colonna di $ w $ scritto nella rispettiva base. 
\end{thm}

\begin{thm}
	Siano $ V $ e $ W $ due spazi vettoriali su $ \K $ di dimensione $ n $ e $ m $. Siano $ \mathscr{B} = \{v_1, \ldots, v_n\} $ e $ \mathscr{B}' = \{w_1, \ldots, w_m\} $ basi rispettivamente di $ V $ e di $ W $. \\ Indicando con $ \mathscr{L}(V, W) = \{L \colon \K^n \to \K^m \quad \mathrm{lineare}\} $ e con $ \left [L \right ]_{\mathscr{B}'} ^{\mathscr{B}} $ la matrice associata a $ L $ rispetto alle basi $ \mathscr{B} $ e $ \mathscr{B}' $, si ha che 
	\begin{align*}
	M \colon \mathscr{L}(V, W) & \to \mathrm{Mat}_{m \times n}(\K)\\
	L & \mapsto \left [L \right ]_{\mathscr{B}'} ^{\mathscr{B}}
	\end{align*}
	è un'applicazione lineare ed è un isomorfismo tra lo spazio delle applicazioni lineari e lo spazio delle matrici. 
\end{thm}

\begin{thm}[matrice di funizione composta]
	Siano $ V $, $ W $ e $ U $ spazi vettoriali e siano \linebreak $ \mathscr{B} = \{v_1, \ldots, v_n\} $, $ \mathscr{B}' = \{w_1, \ldots, w_m\} $ e $ \mathscr{B}'' = \{u_1, \ldots, u_s\} $ basi di $ V $, $ W $ e $ U $ rispettivamente. Siano inoltre $ F \colon V \to W $ e $ G \colon W \to U $ lineari. Allora \[ \left [G \circ F \right ]_{\mathscr{B}''} ^{\mathscr{B}} = \left [G \right ]_{\mathscr{B}''} ^{\mathscr{B}'} \left [F \right ]_{\mathscr{B}'} ^{\mathscr{B}} \]
\end{thm}

\begin{thm}
	Sia $ L \colon V \to W $ un'applicazione lineare e $ B \colon V \to V $ lineare e invertibile. Allora vale che \[\Imm L \circ B = \Imm L \quad \mathrm{e} \quad \dim \Ker L \circ B = \dim  \Ker L\] In altre parole se $ \left [L \right ] $ è la matrice associata a $ L $ e $ \left [B \right ] $ è la matrice invertibile delle mosse di colonna associata a $ B $, la matrice $ \left [B \right ] \left [L \right ] $ è una matrice ridotta a scalini per colonna in cui lo $ Span $ delle colonne è lo stesso dello span delle colonne di $ \left [L \right ] $. Più brevemente la riduzione di Gauss per colonne lascia invariato lo $ Span $ delle colonne. 
\end{thm}

\begin{thm}
	Sia $ L \colon V \to W $ un'applicazione lineare e $ U \colon W \to W $ lineare e invertibile. Allora vale che \[\Ker U \circ L = \Ker L\quad \mathrm{e} \quad \dim \Imm U \circ L = \dim \Imm L  \] In altre parole se $ \left [L \right ] $ è la matrice associata a $ L $ e $ \left [U \right ] $ è la matrice invertibile delle mosse di riga associata a $ U $, la matrice $ \left [L \right ] \left [U \right ] $ è una matrice ridotta a scalini per riga che ha lo stesso $ \Ker $ di $ \left [L \right ] $. Più brevemente la riduzione di Gauss per righe lascia invariato lo spazio delle soluzioni di un sistema lineare omogeneo.
\end{thm}

\begin{definition}[rango]
	Sia $ A \in \mathrm{Mat}_{m \times n} (\K) $ definiamo rango di A, $ \mathrm{rg} A $, in modo equivalente come
	\begin{enumerate}
		\item il numero massimo di colonne linearmente indipendenti (numero di \emph{pivot} colonna di $ A $ ridotta a scalini per colonna)
		\item il numero massimo di righe linearmente indipendenti (numero di \emph{pivot} riga di $ A $ ridotta a scalini per righe)
		\item la $ \dim \Imm L $, dove $ L \colon \K^n \to \K^m $ è l'applicazione lineare associata $ \left [L \right ]_{\mathscr{B}'} ^{\mathscr{B}} = A $
	\end{enumerate}
\end{definition}

\clearpage

\section{Formula di Grassmann e somma diretta}

\begin{thm}[formula di Grassmann]
	Siano $ A $ e $ B $ sottospazi vettoriali di $ V $ su un campo $ \K $. Vale \[\dim A + \dim B = \dim (A + B) + \dim (A \cap B)\]
\end{thm}

\begin{definition}[somma diretta]
	Dati $ A $ e $ B $ sottospazi di $ V $ su un campo $ \K $, si dice che $ A $ e $ B $ sono in somma diretta se $ A \cap B = \{O_V\} $.\\
	In modo del tutto equivalente $ A $ e $ B $ sono in somma diretta se e solo se $ \dim A \, + \, \dim B = \dim (A + B) $.	
\end{definition}

\begin{definition}[somma diretta di $ k $ sottospazi]
	$ U_1, \ldots , U_k $ sottospazi di $ V $ su un campo $ \K $ si dicono essere insomma diretta se $ \forall i \in \{1, \ldots, k\} $ vale \[U_i \cap (U_1 + \ldots + \hat{U}_i + \ldots + U_k) = \{O_V\}\]
	In modo equivalente $ U_1, \ldots , U_k $ sono insomma diretta se e solo se \[\dim U_1 + \ldots + \dim U_k = \dim (U_1 + \ldots + U_k)\]
\end{definition}

\begin{definition}[complementare di un sottospazio]
	Sia $ A $ un sottospazio di $ V $ su un campo $ \K $. Un complementare di $ A $ è un sottospazio $ B $ di $ V $ tale che
	\begin{enumerate}[label=(\roman*)]
		\item $ A \cap B = \{O_V\} $ ($ A $ e $ B $ sono in somma diretta)
		\item $ A + B = V $
	\end{enumerate}
	In tal caso scriveremo che $ A \oplus B = V $. 
\end{definition}

\clearpage

\section{Sistemi lineari}

\begin{definition}[sistema lineare omogeneo]
	contenuto...
\end{definition}

\begin{definition}[matrice associata al sistema lineare omogeneo]
	contenuto...
\end{definition}

\begin{thm}[dimensione delle soluzioni]
	Sia $ M $ la matrice associata ad un sistema lineare omogeneo con $ n $ incognite. Indicando con $ S $ lo spazio delle soluzioni del sistema lineare vale \[\dim S = n - \mathrm{rg}M\]
\end{thm}

\begin{definition}[sottospazio ortogonale]
	contenuto... + \textsf{il sottospazio ortogonale è l'inieme delle soluzioni del sistema omogeneo}
\end{definition}

\begin{definition}[sistema lineare non omogeneo]
	contenuto...
\end{definition}

\begin{definition}[matrice completa e incompleta associata]
	contenuto...
\end{definition}

\begin{thm}[insieme soluzioni del sistema non omogeneo]
	Sia $ S $ l'insieme delle soluzioni del sistema non omogeneo e $ S_0 $ l'insieme delle soluzioni del sistema omogeneo associato. Supposto $ S \neq \emptyset $, preso un qualunque $ v \in S $ vale \[S = v + S_0\]
\end{thm}

\begin{definition}[sottospazio affine]
	\letvs, $ U $ un suo sottospazio e $ v \in V - U $ ($ v \neq 0 $) si dice che l'insieme $ v + U $ è un sottospazio affine di $ V $. Per convenzione si pone $ \dim (v + U) = \dim U $.
\end{definition}

\clearpage

\section{Prodotti scalari}

\begin{definition}[prodotto scalare]
	\letvs. Un prodotto scalare su $ V $ è una funzione \[\scprd{\;}{\,} \colon V \times V \to \K \] che soddisfa le seguenti prorpietà
	\begin{enumerate}[label=(\roman*)]
		\item $ \forall v,w \in V $ vale $ \scprd{v}{w} = \scprd{w}{v} $
		\item $ \forall v, w, u \in V $ vale $ \scprd{v}{w + u} = \scprd{v}{w} + \scprd{v}{u} $
		\item $ \forall v, w \in V, \forall \lambda \in \K $ vale $ \scprd{\lambda v}{w} = \scprd{v}{\lambda w} = \lambda \scprd{v}{w} $
	\end{enumerate}	
\end{definition}

\begin{propriety} Alcuni prodotti scalari godono delle segenti proprietà
	\begin{enumerate}
		\item Un vettore $ v \in V $ tale che $ \scprd{v}{v} = 0 $ si dice \emph{isotropo}.
		\item Un prodotto scalare tale che preso un $ v \in V $ \[\forall w \in V \quad \scprd{v}{w} = O_V \Rightarrow v = O_V\] si dice \emph{non degenere}.
		\item Un prodotto scalare su $ V $ spazio vettoriale sul campo $ \R $ tale che \[ \forall v \in V \quad \scprd{v}{v} \geq 0 \quad \mathrm{e} \quad \scprd{v}{v} = 0 \Leftrightarrow v = O_V \] si dice \emph{definito positivo}.
	\end{enumerate}
\end{propriety}

\begin{thm}
	Un prodotto scalare è non degenere se e solo se la matrice $ E $ che lo rappresenta ha rango massimo. Ovvero, se $ \{e_1, \ldots, e_n \} $ è base di $ V $
	\[ \mathrm{rg} E = \mathrm{rg} 
	\begin{pmatrix}
		\scprd{e_1}{e_1} & \cdots  & \scprd{e_1}{e_n} \\
		\vdots           & \ddots & \vdots \\
		\scprd{e_n}{e_1} & \cdots  & \scprd{e_n}{e_n} \\
	\end{pmatrix}
	= n	\]
\end{thm}

\begin{definition}[norma, distanza, ortogonalità]
	Sia $ \scprd{\,}{} $ un prodotto scalare su $ V $ spazio vettoriale su $ \R $. Allora
	\begin{enumerate}
		\item la norma di $ v \in V $ è $ \norm{v} = \sqrt{\scprd{v}{v}} $
		\item la distanza tra $ v, w \in V $ è data da $ \norm{v - w} $
		\item due vettori $ v, w \in V $ si dicono ortogonali se $ \scprd{v}{w} = 0 $
	\end{enumerate}
\end{definition}

\begin{thm}[di Pitagora]
	\letvsR \, con prodotto scalare definito positivo. Dati $ v, w \in V \colon \scprd{v}{w} = 0 $	allora \[\norm{v + w}^2 = \norm{v}^2 + \norm{w}^2\]
\end{thm}

\begin{thm}[del parallelogramma]
	\letvsR \, con prodotto scalare definito positivo. Allora $ \forall v, w \in V $ vale \[\norm{v + w}^2 + \norm{v - w}^2 = 2\norm{v}^2 + 2\norm{w}^2\]
\end{thm}

\begin{definition}[componente]
	Dati $ v, w \in V $ chiamiamo coefficiente di Fourier o componente di $ v $ lungo $ w $ lo scalare \[c = \frac{\scprd{v}{w}}{\scprd{w}{w}}\] In particolare vale $ \scprd{v - cw}{w} = 0 $
\end{definition}

\begin{thm}[disuguaglianza di Schwarz]
	\letvsR \, con prodotto scalare definito positivo. Allora $ \forall v, w \in V $ vale \[|\scprd{v}{w}| \leq \norm{v} \cdot \norm{w}\]
\end{thm}

\begin{thm}[disuguaglianza triangolare]
	\letvsR \, con prodotto scalare definito positivo. Allora $ \forall v, w \in V $ vale \[\norm{v + w} \leq \norm{v} + \norm{w}\]
\end{thm}

\begin{thm}
	Siano $ v_1, \ldots , v_n \in V $ a due a due perpendicolari ($ \forall i \neq j \; \scprd{v_i}{v_j} = 0 $). Allora $ \forall v \in V $ il vettore \[v - \frac{\scprd{v}{v_1}}{\scprd{v_1}{v_1}}v_1 - \ldots - \frac{\scprd{v}{v_n}}{\scprd{v_n}{v_n}}v_n\] è ortogonale a ciascuno dei $ v_i $. Risulta inoltre che il vettore $ c_1 v_1 + \ldots + c_n v_n $ (dove $ c_i = \frac{\scprd{v}{v_i}}{\scprd{v_i}{v_i}} $) è la migliore approssimazione di $ v $ come combinazione lineare dei $ v_i $.
\end{thm}

\begin{thm}[disuguaglianza di Bessel]
	Siano $ e_1, \ldots , e_n \in V $ a due a due perpendicolari e unitari. Dato un $ v \in V $, sia $ c_i = \frac{\scprd{v}{e_i}}{\scprd{e_i}{e_i}} $. Allora vale \[\sum_{i = 1}^{n}c_i^2 \leq \norm{v}^2\]
\end{thm}

\begin{thm}[ortogonalizzazione di Gram-Schmidt]
	Sia $ V $ uno spazio vettoriale con prodotto scalare definito positivo. Siano $ v_1, \ldots , v_n \in V $ vettori linearmente indipendenti. Possiamo allora trovare dei vettori $ u_1,\ldots, u_r $, con $ r \leq n $ ortogonali tra loro e tali che $ \forall i \leq r, \, Span\{v_1, \ldots, v_i\} = Span \{u_1, \ldots, u_i\} $. In particolare basterà procedere in modo induttivo e prendere
	\[\begin{cases}
	u_1 = v_1 \\
	u_{i} = v_i - \frac{\scprd{v_i}{u_1}}{\scprd{u_1}{u_1}}u_1 - \ldots - \frac{\scprd{v_i}{u_{i-1}}}{\scprd{u_{i-1}}{u_{i-1}}}u_{i-1}
	\end{cases}\]
\end{thm}

\begin{corollary}[esistenza della base ortonormale per prodotto scalare definito positivo]
	Dato $ V $ spazio vettoriale con prodotto scalare definito positivo esiste una base ortonormale di $ V $, ossia una base $ \{u_1,\ldots, u_r\} $ tale che $ \forall i \neq j \; \scprd{u_i}{u_j} = 0 $ e che $ \forall i \; \norm{u_i} = 1 $.
\end{corollary}

\begin{thm}
	Sia $ V $ uno spazio vettoriale con prodotto scalare definito positivo. Per ogni $ W $ sottospazio di $ V $ vale che \[V = W \oplus W^{\perp}\]
\end{thm}

\begin{definition}[prodotto hermitiano]
	\letvsC. Un prodotto hermitiano su $ V $ è una funzione \[\scprd{\;}{\,} \colon V \times V \to \K \] che soddisfa le seguenti proprietà
	\begin{enumerate}[label=(\roman*)]
		\item $ \forall v,w \in V $ vale $ \scprd{v}{w} = \overline{\scprd{w}{v}} $ (coniugato)
		\item $ \forall v, w, u \in V $ vale $ \scprd{v}{w + u} = \scprd{v}{w} + \scprd{v}{u} $ e $ \scprd{v + w}{u} = \scprd{v}{u} + \scprd{w}{u} $
		\item $ \forall v, w \in V, \forall \lambda \in \K $ vale $ \scprd{\lambda v}{w} = \lambda \scprd{v}{w} $ e $ \scprd{v}{\lambda w} = \overline{\lambda} \scprd{v}{w} $
	\end{enumerate}	
\end{definition}

\begin{definition}[prodotto hermitiano standard]
	Dati due vettori colonna $ v, w \in \C^n $ definiamo il prodotto hermitiano standard come 
	\[v \cdot w = 
	\begin{pmatrix}
	\alpha_1 \\
	\vdots \\
	\alpha_n \\ 
	\end{pmatrix}
	\cdot 
	\begin{pmatrix}
	\beta_1 \\
	\vdots \\
	\beta_n \\ 
	\end{pmatrix}
	 = \alpha_1 \overline{\beta_1} + \ldots + \alpha_n \overline{\beta_n}\]
\end{definition}

\begin{thm}[esistenza della base ortogonale per prodotto scalare generico]
	\letvs \ non banale di dimensione finita dotato di un prodotto scalare. Allora $ V $ ha una base ortogonale. 
\end{thm}

\begin{prop}[base ortonormale]
	Sia $ \{w_1, \ldots, w_n\} $ una base ortogonale di $ V $ con un prodotto scalare. Posto
	\[v_i = 
	\begin{cases*}
	\frac{w_i}{\sqrt{\scprd{w_i}{w_i}}} & se $ \scprd{w_i}{w_i} > 0 $ \\
	w_i & se $ \scprd{w_i}{w_i} = 0 $ \\
	\frac{w_i}{\sqrt{-\scprd{w_i}{w_i}}} & se $ \scprd{w_i}{w_i} < 0 $ \\
	\end{cases*}\]
	l'insieme $ \{v_1, \ldots, v_n\} $ è una base ortonormale di $ V $. 
\end{prop}

\begin{thm}[corrispondenza matrice - prodotto scalare]
	\letvs \ e sia $ \mathscr{B} = \{v_1, \ldots, v_n\} $ base di $ V $. Dato un prodotto scale $ \varphi \colon V \times V \to \K $ la matrice del prodotto scalare è \[M_{\mathscr{B}}(\varphi) = (\varphi(v_i, v_j))_{\substack{i = 1, \ldots, n \\ j = 1, \ldots, n}}.\] Viceversa data $ M $ matrice simmetrica del prodotto scalare e $ u, w $ vettori di vettori colonna $ [u]_{\mathscr{B}} $ e $ [w]_{\mathscr{B}} $ si ha \[\varphi(v, w) = [u]_{B}^t \cdot M \cdot [w]_{\mathscr{B}}\].
\end{thm}

\begin{prop}[radicale]
	Vale che $ V_0 = \{v \in V : \forall w \in V, \varphi(v, w) = 0\} = \{v \in V : M_{\mathscr{B}}(\varphi) \cdot [v]_{\mathscr{B}} = 0\} $. (Moralmente $ V_0 = \ker{M_{\mathscr{B}}(\varphi)} $). 
\end{prop}

\begin{definition}[spazio duale, funzionali]
	\letvs. Si definisce spazio duale di $ V $ l'insieme delle applicazioni lineari da $ V $ in $ \K $ \[V^{*} = \mathscr{L}(V, \K) = \mathscr{L}(V) = \{L \colon V \to \K : L \text{ è lineare}\}.\] I suoi elementi vengono detti funzionali lineari da $ V $ in $ \K $ e risulta $ \dim V = \dim V^{*} $. 
\end{definition}

\begin{definition}[base duale]
	\letvs. Fissata una base $ \{v_1, \ldots, v_n\} $ di $ V $ esiste una base $ \{\varphi_1, \ldots, \varphi_n\} $ di $ V^{*} $ ad essa associata detta base duale di $ v_1, \ldots, v_n $ definita come 
	\[\varphi_i (v_j) = 
	\begin{cases*}
	1 & se $ i = j $ \\
	0 & se $ i \neq j $ \\
	\end{cases*}\]
\end{definition}

\begin{thm} \label{thm:isom_duale}
	\letvs \ di dimensione finita con un prodotto scalare non degenere. Allora l'applicazione
	\begin{align*}
	\Phi \colon V & \to V^{*} \\
	v & \mapsto L_v = \scprd{v}{\cdot \,}
	\end{align*}
	dove $ L_v $ è la funzionale tale che $ \forall w \in V : L_v(w) = \scprd{v}{w} $, è un isomorfismo tra $ V $ e il suo duale $ V^{*} $. 
\end{thm}

\begin{thm}[annullatore]
	\letvs \ di $ \dim V = n $ e sia $ W $ sottospazio di $ V $. Sia inoltre $ W^{\circ} = \{\varphi \in V^{*} : \forall w \in W, \varphi(w) = 0\} $ l'annullatore di $ W $. Allora $ W^{\circ} $ è sottospazio di $ V^{*} $ e vale che $ \dim W^{\circ} = n - \dim W $. 
\end{thm}

\begin{corollary}
	\letvs \ di $ \dim V = n $ con prodotto scalare non degenere, sia $ W $ sottospazio di $ V $ e $ W^{\perp} $ il suo ortogonale. Siano inoltre $ V^{*} $ il duale di $ V $ e $ W^{\circ} $ l'annullatore di $ W $. Allora $ \Phi(W^{\perp}) = W^{\circ} $ (in altre parole $ \Phi|_{W^{\perp}} \colon W^{\perp} \to W^{\circ} $ è un isomorfismo) e vale quindi \[\dim W + \dim W^{\perp} = \dim V\]. 
\end{corollary}

\begin{thm}
	Sia $ \Phi \colon V \to V^{*} $ l'isomorfismo del Teorema \ref{thm:isom_duale}. Detto $ V_0 $ il radicale di $ V $ vale che $ V_0 = \ker \Phi $ e che $ M_{\mathscr{B^{*}}}^{\mathscr{B}} (\Phi) = M_{\mathscr{B}}(L_v) $ dove $ \mathscr{B}^{*} $ è la base del duale associata alla base $ \mathscr{B} $ di $ V $. 
\end{thm}

\begin{corollary}
	Un prodotto scalare $ \varphi $ è non degenere $ \iff $ $ V_0 = \{O_V\} $ $ \iff $ $ \ker M_{\mathscr{B}}(\varphi) = O_V $ $ \iff $ $ M_{\mathscr{B}}(\varphi) $ ha rango $ n = \dim V $. 
\end{corollary}

\begin{corollary}
	Se $ \mathscr{B} $ è una base ortonormale allora la matrice del prodotto scalare è diagonale. Detti $ \lambda_i = \varphi(v_i, v_i) $ si ha 
	\[M_{\mathscr{B}}(\varphi) = 
	\begin{pmatrix}
	\lambda_1 & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \ldots & \lambda_n \\
	\end{pmatrix}\]
\end{corollary}

\begin{prop}
	Se $ \mathscr{B} = \{v_1, \ldots, v_n\} $ è una base ortonormale di $ V $ allora \[n_0 (\varphi) = \mathrm{card}\{i : \lambda_i = \varphi(v_i, v_i) = 0\} = \dim V_0.\] Tale valore viene detto indice di nullità del prodotto scalare. 
\end{prop}

\begin{thm}[di Sylvester]
	\letvs \, $ \varphi $ un prodotto scalare su $ V $ e $ \mathscr{B} = \{v_1, \ldots, v_n\} $ una base di $ V $. Esiste un numero intero $ r = n_{+} (\varphi) $ che dipende solo da $ \varphi $ e non dalla base $ \mathscr{B} $, detto indice di positività, tale che ci sono esattamente $ r $ indici $ i $ tali che $ \varphi(v_i, v_i) = 1 $. Analogamente esiste un $ r' = n_{-} (\varphi) $ che dipende solo da $ \varphi $ e non dalla base $ \mathscr{B} $, detto indice di negatività, tale che ci sono esattamente $ r' $ indici $ i $ tali che $ \varphi(v_i, v_i) = - 1 $.
\end{thm}

\begin{definition}[segnatura]
	Si definisce segnatura di un prodotto scalare $ \varphi $ su $ V $ la terna \linebreak $ (n_0 (\varphi), n_{+} (\varphi), n_{-} (\varphi)) $. Detta $ n = \dim V $ vale \[n_0 (\varphi) + n_{+} (\varphi) + n_{-} (\varphi) = n.\] Dato inoltre $ W $ sottospazio vettoriale di $ V $ valgono le seguenti caratterizzazioni
	\begin{itemize}
		\item $ n_0 (\varphi) = \dim V_0 $ (radicale);
		\item $ n_{+} (\varphi) = \max{\{\dim W : W \subseteq V \text{ e } \varphi|_{W} > 0\}} $;
		\item $ n_{-} (\varphi) = \max{\{\dim W : W \subseteq V \text{ e } \varphi|_{W} < 0\}} $.
	\end{itemize}
\end{definition}

\clearpage

\section{Determinante}

\begin{definition}[gruppo simmetrico]
	Il gruppo simmetrico di un insieme è il gruppo formato dall'insieme delle permutazioni dei suoi elementi, cioè dall'insieme delle funzioni biiettive di tale insieme in se stesso, munito dell'operazione binaria di composizione di funzioni. \\
	In particolare detto $ S_n = \{1, \ldots, n\} $ l'insieme delle permutazioni \[\Sigma_n = \Sigma (S_n) = \{\sigma \colon S_n \to S_n : \sigma \text{ è biettiva}\}\] è un gruppo simmetrico. Ricordiamo che una permutazione $ \sigma \in \Sigma_n $ viene spesso indicata con la seguente notazione 
	\[\begin{pmatrix}
	1 & 2 & \cdots & n \\
	\sigma (1) & \sigma (2) & \cdots & \sigma (n) \\
	\end{pmatrix}\]
	dove si intende che l'elemento $ i $ viene mandato in $ \sigma (i) $ dalla permutazione. 
\end{definition}

\begin{definition}[trasposizione]
	Una trasposizione è una permutazione $ \tau \in \Sigma_n $ tale che scambia due soli elementi di $ S_n $ mentre lascia invariati i restaniti $ n - 2 $. Se $ \tau $ scambia $ i, j \in S_n $ scriveremo $ \begin{pmatrix}
	i & j \\
	\end{pmatrix} $. 
\end{definition}

\begin{prop}
	\begin{enumerate}[label = (\roman*)]
		\item Ogni permutazione $ \sigma \in \Sigma_n $ è esprimile, non in modo unico, come prodotto (composizione) di trasposizioni. 
		\item Se $ \sigma = \tau_1 \circ \cdots \circ \tau_h = \lambda_1 \circ \cdots \circ \lambda_h $ con $ \tau_i $ e $ \lambda_j $ trasposizioni allora $ h $ e $ k $ hanno la stessa parità. Se $ \sigma $ è prodotto di un numero pari (dispari) di trasposizioni diremo che $ \sigma $ è pari (dispari). 
	\end{enumerate}
\end{prop}

\begin{definition}
	Data $ \sigma \in \Sigma_n $ definiamo la funzione segno $ \sgn \colon \Sigma_n \to \{-1, 1\} $ come 
	\[\sgn(\sigma) = 
	\begin{cases*}
	1 & se $ \sigma $ è pari \\
	-1 & se $ \sigma $ è dispari \\
	\end{cases*}\]
	Vale in particolare che $ \sgn(\sigma_1 \circ \sigma_2) = \sgn(\sigma_1) \cdot \sgn(\sigma_2) $
\end{definition}

\begin{prop}
	Sia $ \sigma \in \Sigma_n $ una permutazione e sia $ \sigma^{-1} $ la permutazione inversa. Allora $ \sgn(\sigma) = \sgn(\sigma^{-1}) $. 
\end{prop}

\begin{thm}[Unicità del determinante]
	Sia $ \mathrm{Mat}_{n \times n} (\K) $  lo spazio vettoriale delle matrici quadrate  a valori nel campo $ \K $. Esiste una ed una sola funzione da $ \mathrm{Mat}_{n \times n} (\K) $ in $ \K $ funzione delle righe (o delle colonne) di una matrice $ A $ che rispetta i seguenti tre assiomi:  
	\begin{enumerate}[label = (\roman*)]
		\item \emph{multilineare} (lineare in ogni riga o colonna);
		\item \emph{alternante} (cambia di segno se si scambiano due righe o due colonne);
		\item \emph{normalizzata} (l'immagine dell'identità è 1);
	\end{enumerate}
	Tale funzione viene detta determinante ed indicata con $ \det \colon \mathrm{Mat}_{n \times n} (\K) \to \K $. 
\end{thm}

\begin{propriety}[del determinante]
	Le seguenti proprietà sono conseguenza degli assiomi (i), (ii) e (iii). Sia $ A \in \mathrm{Mat}_{n \times n} (\K) $ allora
	\begin{enumerate}[label = (\arabic*)]
		\item Se $ A $ ha due righe uguali allora $ \det{A} = 0 $.
		\item Se $ A $ ha una riga nulla allora $ \det{A} = 0 $.
		\item Se alla riga $ A_i $ di $ A $ si somma un multiplo della riga $ A_j $ ($ i \neq j $) si ottiene una matrice $ B $ tale che $ \det A = \det B $. 
		\item Il determinante è invariante sotto l'algoritmo di Gauss (escludendo le mosse di \emph{normalizzazione} delle righe o delle colonne) a meno di un segno che dipende dal numero di scambi di righe o di colonne fatto. In altre parole se $ S $ è una forma a scalini di $ A $ allora $ \det A = \pm \det S $.
		\item Se $ A $ è una matrice diagonale allora il suo determinante è il prodotto degli elementi sulla diagonale: $ \det A = a_{11} \cdots a_{nn} $. 
		\item $ A $ è invertibile $ \iff $ $ \det A \neq 0 $ ($ \iff $ $ \rg A = n $). 
	\end{enumerate}
\end{propriety}

\begin{thm}[esistenza del determinante]
	Sia $ A = (a_{ij})_{\substack{i = 1, \ldots, n \\ j = 1, \ldots, n}} \in \mathrm{Mat}_{n \times n} (\K) $. La funzione \[\det(A) = \sum_{\sigma \in \Sigma_n} \sgn(\sigma) \cdot a_{1 \sigma(1)} a_{2 \sigma(2)} \cdots a_{n \sigma{n}}\] è il determinante (in quanto è una funzione multilineare, alternante e normalizzata dallo spazio delle matrici nel campo). 
\end{thm}

\begin{corollary}
	Il determinante di $ A $ è uguale al determinante della sua trasposta: $ \det A = \det A^{t} $. 
\end{corollary}

\begin{definition}[complemento algebrico]
	Il complemento algebrico o cofattore dell'elemento $ a_{ij} $ di una matrice $ A \in \mathrm{Mat}_{n \times n} (\K) $ è il determinante della matrice $ (n - 1) \times (n - 1) $ ottenuta cancellando da $ A $ la $ i $-esima riga e la $ j $-esima colonna moltiplicato per $ (-1)^{i + j} $: in formule
	\[\cof_{ij}(A) = (-1)^{i + j} \cdot \det 
	\begin{pmatrix}
		a_{11} & \cdots & \cancel{a_{1j}} & \cdots & a_{1n} \\
		\vdots &        & \vdots &        & \vdots \\
		\cancel{a_{i1}} & \cdots & \cancel{a_{ij}} & \cdots & \cancel{a_{in}} \\
		\vdots &  		& \vdots &  	  & \vdots \\
		a_{n1} & \cdots & \cancel{a_{nj}} & \cdots & a_{nn} \\
	\end{pmatrix}\]
\end{definition}

\begin{thm}[sviluppo di Laplace]
	Data $ A \in \mathrm{Mat}_{n \times n} (\K) $ la seguente funzione
	\begin{itemize}
		\item fissata una riga $ i $ di $ A $: $ \det A = \sum_{j = 1}^{n} a_{ij} \cdot \cof_{ij} (A) $
		\item fissata una colonna $ j $ di $ A $: $ \det A = \sum_{i = 1}^{n} a_{ij} \cdot \cof_{ij} (A) $
	\end{itemize}
	verifica gli assiomi (i), (ii) e (iii) e quindi è il determinante. 
\end{thm}

\clearpage

\section{Diagonalizzazione, autovettori e autovalori}

\end{document}
